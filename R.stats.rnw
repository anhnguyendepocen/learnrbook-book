% !Rnw root = appendix.main.Rnw
<<echo=FALSE, include=FALSE>>=
opts_chunk$set(opts_fig_wide)
opts_knit$set(concordance=TRUE)
opts_knit$set(unnamed.chunk.label = 'functions-chunk')
@

\chapter{The R language: Statistics}\label{chap:R:functions}

\begin{VF}
The purpose of computing is insight, not numbers.

\VA{Richard W. Hamming}{Numerical Methods for Scientists and Engineers, 1962}
\end{VF}

\section{Aims of this chapter}

In this chapter you will learn the approach used in \Rlang for calculating statistical summaries, generating (pseudo-)random numbers, sampling, fitting models and carrying out tests of significance. We will use linear correlation, \emph{t}-test, linear models, generalized linear model, non-linear models and some simple multivariate methods as examples. My aim is teaching how to specify models, contrasts and data used, and how to access different components of the objects returned by the corresponding functions.

This chapter is designed to give the reader only a quick introduction to statistics in base \Rlang, as there are many good texts on the use of \Rpgrm for different kinds of statistical analyses. Three good examples of books with a broad scope and of moderate size are \citebooktitle{Dalgaard2008} \autocite{Dalgaard2008}, \citebooktitle{Everitt2009} \autocite{Everitt2009} and \citebooktitle{??} \autocite{??}. The book \citebooktitle{Crawley2012} \autocite{Crawley2012} is comprehensive in the scope of statistical procedures described. Furthermore, many of base \R's functions are specific to different statistical procedures and transcend the description of \Rlang as a programming language.

Along the chapter, I will show occasionally the equivalent of \Rlang code in mathematical notation. If you are not familiar with the mathematical notation, you can safely ignore it, as long as you understand the \Rlang code.

\emph{\textcolor[rgb]{1.00,0.50,0.25}{As a stop-gap we use examples from the help pages of several of the functions described.}}

\section{Statistical summaries}
\index{functions!built-in|see@{functions, base-R}}
\index{functions!base R}
Being R's main focus in statistics, it provides functions for both simple and complex calculations, going from means and variances to fitting very complex models. Below are examples of functions implementing the calculation of frequently used data summaries. \qRfunction{mean()}\qRfunction{var()}\qRfunction{median()},\qRfunction{mad()}\qRfunction{sd()}\qRfunction{range()}\qRfunction{max()}\qRfunction{min()}\qRfunction{length()}

<<stat-fun-1, eval=eval_playground>>=
x <- 1:20
mean(x)
var(x)
median(x)
mad(x)
sd(x)
range(x)
max(x)
min(x)
length(x)
@

\begin{playground}
  In contrast to many other examples in this book, the summaries computed with the code in the previous chunk are not shown. You should \emph{run} them, using vector \code{x} as defined above, and then play with other real or artificial data that you may find interesting.% Later in the book, only the output from certain examples will be shown, with the expectation, that other examples will be run by readers.
\end{playground}

\section{Distributions}

Density, distribution functions, quantile functions and random generation for several different distributions are part of the \Rlang language. Entering \code{help(Distributions)} at the \Rlang prompt will display a page displaying all the distributions available in base Rlang. In what follows we use the Normal distribution for the examples, but with slight differences in there parameters the remaining functions follow the same naming pattern.

\subsection{Distribution from parameters}

<<distrib-01>>=
plot(function(x) dnorm(x, mean = 1, sd = 0.5), -1, 3)
@

\subsection{Probabilities from parameters and quantile}

<<distrib-02>>=
pnorm(q = 4, mean = 0, sd = 1)
pnorm(q = 4, mean = 0, sd = 1, lower.tail = FALSE)
pnorm(q = 4, mean = 0, sd = 4, lower.tail = FALSE)
pnorm(q = c(2, 4), mean = 0, sd = 1, lower.tail = FALSE)
pnorm(q = 4, mean = 0, sd = c(1, 4), lower.tail = FALSE)
@

\subsection{Quantiles from parameters and probabilities}

<<distrib-03>>=
qnorm(p = 0.33, mean = 0, sd = 1)
qnorm(p = 0.33, mean = 0, sd = 1, lower.tail = FALSE)
@

\subsection{Random draw from a distribution}

rnorm(n = 10, mean = 10, sd = 2)
<<distrib-04>>=
rnorm(n = 10, mean = 10, sd = 2)
@

\begin{playground}
Edit some of the examples above for different distributions, such at Students' t, F or uniform.
\end{playground}

\section{Random sampling}

<<sampling-01>>=
sample(x = LETTERS)
sample(x = LETTERS, size = 12)
sample(x = LETTERS, size = 12, replace = TRUE)
@

\begin{explainbox}
It is impossible to generate truly random sequences of number by means of a deterministic process such as a mathematical computation. ``Random numbers'' as generated by \Rpgrm and other computer programs are \emph{pseudo random numbers}, long deterministic series of numbers that resemble random draws. Random number generation uses a \emph{seed} value that determines where in the series we start. The usual of automatically setting the value of the seed is to take the milliseconds or similar rapidly changing set of digits from the real time clock of the computer. However, in cases when we wish to repeat a calculation using the same series of ``random'' values, we can use \Rfunction{set.seed()} with an arbitrary integer as argument to reset the generator.
\end{explainbox}

\begin{advplayground}
Execute the statement \code{rnorm(5)} by itself several times, paying attention to the values obtained. Repeat the exercise but now executing \code{setseed(98765); rnorm(5)} several times, paying attention to the values now obtained, then try again using a different argument in the call to \code{setseed()}.
\end{advplayground}

\section{Correlation}

Both parametric and non-parametric robust methods for the estimation of the (linear) correlation between pairs of variables can be easily computed in \Rlang.

\subsection{Pearson's $r$}

<<cor-01>>=
cor(cars)
@

<<cor-02>>=
cor(x = cars$speed, y = cars$dist)
@

<<cor-03>>=
cor.test(x = cars$speed, y = cars$dist)
@

\begin{playground}
Functions \Rfunction{cor()} and \Rfunction{cor.test()} return \Rlang objects, that when using \Rlang interactively get automatically ``printed'' on the screen. One should be aware that \Rfunction{print()} methods do not necessarily display all the information contained in an \Rlang object. This is almost always the case for complex objects like those returned by \Rlang functions implementing statistical tests. As with any \Rlang object we can save the result of an analysis into a variable. As described in section \ref{sec:calc:lists} on page \ref{sec:calc:lists} for lists, we can peek into the structure of an object with method \Rfunction{str()}. We can use \Rfunction{class()} and \Rfunction{attributes()} to extract further information. Run the code in the chunk below to discover what is actually returned by \Rfunction{cor()}.

<<cor-PG01, eval=eval_playground>>=
a <- cor(cars)
class(a)
attributes(a)
str(a)
@

Methods \Rfunction{class()}, \Rfunction{attributes()} and \Rfunction{str()} are very powerful tools that can be used when we are in doubt of the data contained in an object and/or how it is structured. Knowing the structure allows us to retrieve the data directly from the object.
\end{playground}

\subsection{Kendall's $\tau$ and Spearman's $\rho$}

We use the same functions as for Pearson's $r$ but explicitly request the use of one of these methods.

<<cor-04>>=
cor(x = cars$speed, y = cars$dist, method = "kendall")
cor(x = cars$speed, y = cars$dist, method = "spearman")
@

Function \code{cor.test()} allows the choice of the method using the same syntax as for \code{cor()}.

\begin{playground}
Repeat the exercise in the playground immediately above, but now using non-default methods as argument to \Rfunction{cor()}. How does the information stored in the returned \code{matrix} differ depending on the method, and how can we extract information about the method used for calculation of the correlation from the returned object.
\end{playground}

\section{Fitting linear models}\label{sec:stat:LM}
\index{models!linear|see{linear models}}
\index{linear models|(}
\index{LM|see{linear models}}

In \Rlang, the models to be fitted are described by ``model formulas'' such \verb|y ~ x| which we read as $y$ is explained by $x$. Model `formulas' are used in different contexts: fitting of models, plotting, and tests like $t$-test. The syntax of model formulas is consistent throughout base \Rlang and numerous independently developed packages. However, their use is not universal, and several packages extend the basic syntax to allow the description of specific types of models.

As most things in \Rlang model formulas can be stored in variables. In addition, contrary to the usual behaviour of other statistical software, the result of a model fit is returned as an object, containing the different components of the fit. Once the model has been fitted, different methods allow us to extract parts and/or further manipulate the result of fitting the model. Most of these methods have implementations for model fit objects for many different types of statistical models. Consequently what is described in this chapter using linear models as examples, also applies in many respects to the fit of classes of models not described here.

The \Rlang function \Rfunction{lm()} is used next to fit linear models. If the explanatory variable is continuous, the fit is a regression. If the explanatory variable is a factor, the fit is an analysis of variance (ANOVA) in broad terms. However, there is another meaning of ANOVA, referring only to the tests of significance rather to an approach to model fitting. Consequently, rather confusingly, results for tests of significance for fitted parameter estimates can both in the case of regression and ANOVA, be presented in an ANOVA table. In this second, stricter meaning, ANOVA means a test of significance based on the ratio between two variances.

\begin{warningbox}
If you do not clearly remember the difference between numeric vectors and factors, or how they can be created, please, revisit Chapter \ref{chap:R:as:calc} on page \pageref{chap:R:as:calc}.
\end{warningbox}

\subsection{Regression}
\index{linear regression}\index{linear models!linear regression}
In the example immediately below, \code{speed} is a continuous numeric variable. In the ANOVA table calculated for the model fit, in this case a linear regression, we can see that the term for \code{speed} has only one degree of freedom (df) for the denominator.

We first load and explore the data to which we will fit a linear model, data that is included in \Rpgrm. These data consist in stopping distances for cars moving at different speeds as described in the documentation available by entering \code{help(cars)}).
\label{xmpl:fun:lm:fm1}

<<models-1z>>=
data(cars)
plot(cars)
is.factor(cars$speed)
is.numeric(cars$speed)
@

We then fit the simple linear model $y = \alpha \cdot 1 + \beta \cdot x$ where $y$ corresponds to stopping distance (\code{dist}) and $x$ to initial speed (\code{speed}). Such a model is formulated in \Rlang as \verb|dist ~ 1 + speed|. We save the fitted model as \code{fm1} (a mnemonic for fitted-model one).%
<<models-1>>=
fm1 <- lm(dist ~ 1 + speed, data=cars)
@

The next step is diagnosis of the fit. Are assumptions of the linear model procedure used reasonably fulfilled? In \Rlang it is most common to use plots to this end. We show here only one of the four plots normally produced. This quantile vs.\ quantile plot allows to assess how much the residuals deviate from being normally distributed.

<<models-1a>>=
plot(fm1, which = 2)
@

In the case of a regression, calling \Rfunction{summary()} with the fitted model object as argument is most useful as it provides a table of coefficient estimates and their errors.
<<models-1b>>=
summary(fm1)
@

Let's look at the summary, section by section. Under ``Call:'' we find, \verb|dist ~ 1 + speed| or the specification of the model fitted, plus the data used. Under ``Residuals:'' we find the extremes, quartiles and median of the residuals, or deviations between observations and the fitted line. Under ``Coefficients:'' we find the estimates of the model parameters and their variation plus corresponding $t$-tests. At the end of summary there is information on degrees of freedom and overall coefficient of determination ($R^2$).

If we return to the model formulation, we can now replace $\alpha$ and $\beta$ by the estimates obtaining $y = -17.6 + 3.93 x$. Given the nature of the problem, we \emph{know based on first principles} that stopping distance must be zero when speed is zero. This suggests that we should not estimate the value of $\alpha$ but instead set $\alpha = 0$, or in other words fit the model $y = \beta \cdot x$.

However, in \Rlang models the intercept is always implicitly included, so the model fitted above can be formulated as \verb|dist ~ speed|---i.e.\ a missing \code{+1} does not change the model. To `remove' the intercept from the earlier model we need to use \verb|dist ~ speed - 1|, resulting in the fitting of a straight line through the origin ($x = 0$, $y = 0$).

<<models-2>>=
fm2 <- lm(dist ~ speed - 1, data=cars)
summary(fm2)
@

Now there is no estimate for the intercept in the summary, only an estimate for the slope. The equation of the second fitted model is $y = 2.91 x$, and from the residuals, it can be seen that it is inadequate, as the straight line does not follow the curvature of the relationship between \code{dist} and \code{speed}.

<<models-2a>>=
plot(fm2, which = 1)
@

\begin{playground}
You will now fit a second degree polynomial\index{linear models!polynomial regression}\index{polynomial regression}, a different linear model: $y = \alpha \cdot 1 + \beta_1 \cdot x + \beta_2 \cdot x^2$. The function used is the same as for linear regression, \Rfunction{lm()}. We only need to alter the formulation of the model. The identity function \Rfunction{I()} is used to protect its argument from being interpreted as part of the model formula. Instead, its argument is evaluated beforehand and the result is used as the, in this case second, explanatory variable.

<<models-3, eval=eval_playground>>=
fm3 <- lm(dist ~ speed + I(speed^2), data = cars)
plot(fm3, which = 3)
summary(fm3)
anova(fm3)
@

The ``same'' fit using an orthogonal polynomial. Higher degrees can be obtained by supplying as second argument to \Rfunction{poly()} a different positive integer value.

<<models-3a, eval=eval_playground>>=
fm3a <- lm(dist ~ poly(speed, 2), data=cars)
summary(fm3a)
anova(fm3a)
@

We can also compare two model fits using \Rfunction{anova()}, to test whether one of models describes the data better than the other.

<<models-4, eval=eval_playground>>=
anova(fm2, fm1)
@

Or three or more models. But be careful, as the order of the arguments matters.

<<models-5, eval=eval_playground>>=
anova(fm2, fm1, fm3, fm3a)
@

We can use different criteria to choose the best model: significance based on $P$-values or information criteria (AIC, BIC). AIC (Akaike's ‘An Information Criterion’) and BIC (= SBC, Schwarz's Bayesian criterion) that penalize the resulting `goodness' based on the number of parameters in the fitted model. In the case of AIC and BIC, a smaller value is better, and values returned can be either positive or negative, in which case more negative is better.\qRfunction{BIC()}\qRfunction{AIC()}

<<models-5a, eval=eval_playground>>=
BIC(fm2, fm1, fm3, fm3a)
AIC(fm2, fm1, fm3, fm3a)
@

Once you have run the code in the chunks above, you will be able see that these three criteria not necessarily agree on which is the ``best'' model. Find in the output $p$-values from ANOVA, BIC and AIC values, for the different models and conclude on which model is favoured by each of the three criteria. In addition you will notice that the two different formulations of the quadratic polynomial are equivalent.

\end{playground}

\begin{advplayground}
Additional methods provide access to different components of fitted models: \Rfunction{vcov()} returns the variance-covariance matrix, \Rfunction{coef()} and its alias \Rfunction{coefficients()} return the estimates for the fitted model coefficients, \Rfunction{fitted()} and its alias \Rfunction{fitted.values()} extract the fitted values, and \Rfunction{residuals()} the corresponding residuals (or deviations). The method \Rfunction{predict()} uses the fitted model to compute predictions and confidence bands based from new data for the independent variables.

Task: familiarize yourself with these methods by reading the documentation and using them with the models fitted above or model fits to other data of your interest.
\end{advplayground}

\begin{explainbox}
The objects returned by model fitting functions are rather complex and contain the full information, including the data to which the model was fit to. The different functions described above, either extract parts of the object or do additional calculations and formatting based on them.

We rarely need to manually explore the structure of these objects, but it is rather frequently good to have an idea of what is going on in the background. We can use \Rfunction{str()} to explore some of the fitted models.

<<models-EB1, eval=FALSE>>=
str(fm1, max.level = 1) # not evaluated
@

We frequently only look at the output of \Rfunction{anova()} as implicitly displayed by \code{print()}. However, \Rfunction{anova()} and \Rfunction{summary()} return complex objects containing different members with data. Understanding this is frequently useful, when we want to either display the results in a different format, or extract parts of them. Once again we use \Rfunction{str()}.

<<models-EB2>>=
str(anova(fm1))
@

<<models-EB3>>=
str(summary(fm1))
@

Once we know the nesting and the names of components we can simply extract them using indexing.

<<models-EB4>>=
summary(fm1)$adj.r.squared
@

As an example we test if the slope from a linear regression fit deviates significantly from a constant value different from the usual zero.

The examples above are for a null hypothesis of slope = 0 and next we show how to do the equivalent test with null hypothesis of slope = 1. The procedure is applicable to any constant value as null hypothesis for any of the fitted parameter estimates set \emph{a priori}. The examples use a two-sided test, in some cases, a single-sided test should be used (e.g.\ if its known a priori that deviation is possible only in one direction away from the hypothesis).

To estimate the \emph{t}-value we need the estimate for a parameter and an estimate of the standard error for this estimate and its degrees of freedom.
<<models-EB5>>=
est.slope.value <- summary(fm1)$coef["speed", "Estimate"]
est.slope.se <- summary(fm1)$coef["speed", "Std. Error"]
degrees.of.freedom <- summary(fm1)$df[2]
@

The \emph{t}-test is based on the difference between the value of the null hypothesis and the estimate.

<<models-EB6>>=
hyp.null <- 1
t.value <- (est.slope.value - hyp.null) / est.slope.se
p.value <- dt(t.value, df = degrees.of.freedom)
@
\end{explainbox}

\begin{advplayground}
Check that the procedure above agrees with the output of \code{summary()} for \code{hyp.null <- 0}.

Modify the example so as to test whether the intercept is significantly different from 5 feet.
\end{advplayground}

\subsection{Analysis of variance, ANOVA}\label{sec:anova}
\index{analysis of variance}\index{linear models!analysis of variance}
\index{ANOVA|see{analysis of variance}}
We use as the \code{InsectSprays} data set, giving insect counts in plots sprayed with different insecticides. In these data \code{spray} is a factor with six levels.%
\label{xmpl:fun:lm:fm4}

The call is exactly the same as the one for linear regression, only the names of the variables and data frame are different. What determines that this is an ANOVA is that \code{spray}, the explanatory variable, is a \code{factor}.

<<models-6z>>=
data(InsectSprays)
is.factor(InsectSprays$spray)
is.numeric(InsectSprays$spray)
@

<<models-6>>=
fm4 <- lm(count ~ spray, data = InsectSprays)
@

<<model-6a>>=
plot(fm4, which = 3)
@

<<model-6b>>=
anova(fm4)
@

In ANOVA we are mainly interested in testing hypotheses, and \Rfunction{anova()} provides the most interesting output. Function \Rfunction{summary()} can be used to extract the estimates but usually the default contrasts and corresponding $P$-values are for hypotheses that have little or no direct interest.

\subsection{Analysis of covariance, ANCOVA}
\index{analysis of covariance}\index{linear models!analysis of covariance}
\index{ANCOVA|see{analysis of covariance}}

When a linear model includes both explanatory factors and continuous explanatory variables, we may call it \emph{analysis of covariance} (ANCOVA). The formula syntax is the same for all linear models, what determines the type of analysis is the nature of the explanatory variable(s). Conceptually a factor (an unordered categorical variable) is very different from a continuous variable.

As the formulation remains the same, no specific example is given. The main difficulty of ANCOVA is in the selection of the covariate and the interpretation of the results of the analysis \autocite{}.

\begin{playground}
There are additional methods for extracting information from model fits. As objects containing the results of model fitting belong to different classes reflecting the type of model, methods with the same name can co-exist and be selected automatically. The output will not be necessarily the same, and some the parameters accepted will differ.\qRfunction{coef()}\qRfunction{resid()}

<<model-PG01,eval=eval_playground>>=
class(fm1)
coef(fm1)
resid(fm1)
@

Do explore these methods if you know enough about statistics to recognize them. In the case of prediction with \Rfunction{predict()} new data needs to be supplied as argument as in the case of linear models no default is provided for this parameter, which in other cases are the data used in the fit. Be aware that the name of the variable (= column) within the new data frame needs to match the name of the explanatory variable in the data to which the model was fit, in our case \code{speed}.

<<model-PG02a,eval=eval_playground>>=
my.new.data <- data.frame(speed = 5:10)
@

<<model-PG02b,eval=eval_playground>>=
predict(fm1, newdata = my.new.data)
@

<<model-PG02c,eval=eval_playground>>=
predict(fm1, newdata = my.new.data, interval = "confidence", level = 0.9)
@

Some other components of the fitted model object can be extracted with the usual syntax for lists, once we discover the names of the components. As shown earlier, we can use \code{str()} or \code{names()} to this end.

<<model-PG03a,eval=eval_playground>>=
names(fm1)
@

<<model-PG03c,eval=eval_playground>>=
fm1$df.residual
@

To see the whole structure of the fitted model object we can use \code{str()}, which reveals the nesting of members and all their \emph{attributes} (described in section \ref{sec:calc:attributes} on page \pageref{sec:calc:attributes}). In this case attributes contain very important information about the fit.

<<model-PG04,eval=eval_playground>>=
str(fm1, max.level = 1)
@

\end{playground}
\index{linear models|)}

\section{Generalized linear models}\label{sec:stat:GLM}
\index{generalized linear models|(}\index{models!generalized linear|see{generalized linear models}}
\index{GLM|see{generalized linear models}}

Linear models make the assumption of normally distributed residuals. Generalized linear models, fitted with function \Rfunction{glm()} are more flexible, and allow the assumed distribution to be selected as well as the link function.
For the analysis of the \code{InsectSpray} data set, above (section \ref{sec:anova} on page \pageref{sec:anova}) the Normal distribution is not a good approximation as count data deviates from it. This was visible in the quantile--quantile plot above.

For count data GLMs provide a better alternative. In the example below we fit the same model as above, but we assume a quasi-Poisson distribution instead of the Normal. In addition to the model formula we need can pass an argument through \code{family} giving the error distribution to be assumed---the default for \code{family} is \code{gaussian} or Normal distribution.

<<model-10>>=
fm10 <- glm(count ~ spray, data = InsectSprays, family = quasipoisson)
anova(fm10)
@

The print-out from the \Rfunction{anova()} method for GLM fits has some differences to that for LM fits. By default no significance test is computed, as a knowledgable choice is required depending on the characteristics of the model and data. We here use \code{"F"} as argument to request an $F$-test.

<<model-10a>>=
anova(fm10, test = "F")
@

Method \Rfunction{plot()} as for LM fits, produces diagnosis plots. We show as above the q-q-plot of residuals.

<<model-11>>=
plot(fm10, which = 3)
@
\index{generalized linear models|)}

\section{Non-linear regression}\label{sec:stat:NLS}
\index{non-linear models|(}\index{models!non-linear|see{non-linear models}}

Function \Rfunction{nls()} is \Rlang's workhorse for fitting non-linear models. By \emph{non-linear} it is meant non linear \emph{in the parameters} whose values is being estimated through fitting the model to data. This is different from the shape of the function when plotted---i.e.\ polynomials of any degree are linear models.

In the case of \Rfunction{nls()} the specification of the model to be fitted differs from that used for linear models. We will use as example fitting the Michaelis-Menten equation describing reaction kinetics in biochemistry and chemistry. The mathematical formulation is given by:

\begin{equation}\label{eq:michaelis:menten}
v = \frac{\mathrm{d} [P]}{\mathrm{d} t} = \frac{V_{\mathrm{max}} [S]}{K_{\mathrm{M}} + [S]}
\end{equation}

\index{non-linear models|)}

\section{Model formulas}
  In the examples above we fitted simple models. More complex ones can be easily formulated using the same syntax. First of all one can avoid use of \code{*} and explicitly define all individual main effects and interactions. The syntax implemented in base \Rlang allows grouping by means of parentheses, so it is also possible to use grouping to exclude some interactions.

The same symbols as for arithmetic operators are used for model formulas. Within a formula, symbols are interpreted according to formula syntax. When we mean an arithmetic operation that could be interpreted as being part of the model formula we need to ``protect'' it by means of the identity function \Rfunction{I()}. The next two examples define formulas for models with only one explanatory variable. With formulas like these the explanatory variable will be computed on the fly when fitting the model to data. In the first case below we need to explicitly protect the addition of the two variables into their sum, because otherwise they would be interpreted as two explanatory variables in the model. In the second case \Rfunction{log()} cannot be interpreted as part of the model formula, and consequently does no require additional protection, neither does its argument.

<<mformulas-000,eval=FALSE>>=
y ~ I(x1 + x2)
y ~ log(x1 + x2)
@

\Rlang's formula syntax allows alternative ways for specifying interaction terms. They allow ``abbreviated'' ways of entering formulas, which for complex experimental designs saves typing and can improve clarity. As seen above operator \code{*} saves us from having to explicitly indicate all the interaction terms in a full factorial model.

<<mformulas-00,eval=FALSE>>=
y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3
@

Can be replaced by a concise equivalent.

<<mformulas-00a,eval=FALSE>>=
y ~ x1 * x2 * x3
@

When the model to be specified does not include all possible interaction terms, we can combine the concise notation with parentheses.

<<mformulas-01,eval=FALSE>>=
y ~ x1 + (x2 * x3)
y ~ x1 + x2 + x3 + x2:x3
@

That the two model formulas above are equivalent, can be seen using \code{term()}

<<mformulas-01a>>=
terms(y ~ x1 + (x2 * x3))
@

<<mformulas-02,eval=FALSE>>=
y ~ x1 * (x2 + x3)
y ~ x1 + x2 + x3 + x1:x2 + x1:x3
@

<<mformulas-02a>>=
terms(y ~ x1 * (x2 + x3))
@

The \code{\textasciicircum{}} operator can be used to limit the order of the interaction terms included in a formula.

<<mformulas-03,eval=FALSE>>=
y ~ (x1 + x2 + x3)^2
y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3
@

<<mformulas-03a>>=
terms(y ~ (x1 + x2 + x3)^2)
@

\begin{playground}
 For operator \code{\textasciicircum{}} to behave as expected its first operand should be a formula with no interactions!  Compare the result of expanding these two formulas.

<<mformulas-PG01, eval=eval_playground>>=
y ~ (x1 + x2 + x3)^2
y ~ (x1 * x2 * x3)^2
@

\end{playground}

Operator \code{\%in\%} can also be used as a shortcut to including only some of all the possible interaction terms in a formula.

<<mformulas-04,eval=FALSE>>=
y ~ x1 + x2 + x1 %in% x2
@

<<mformulas-04a>>=
terms(y ~ x1 + x2 + x1 %in% x2)
@

\begin{playground}
The following examples of the use of formulas in ANOVA, plus your own variations on the same theme will help understand the syntax of model formulas.

<<mformulas-PG02, eval=eval_playground>>=
data(npk)
anova(lm(yield ~ N * P * K, data = npk))
@

<<mformulas-PG03, eval=eval_playground>>=
anova(lm(yield ~ (N + P + K)^2, data = npk))
@

<<mformulas-PG04, eval=eval_playground>>=
anova(lm(yield ~ N + P + K + P %in% N + K %in% N, data = npk))
@

<<mformulas-PG05, eval=eval_playground>>=
anova(lm(yield ~ N + P + K + N %in% P + K %in% P, data = npk))
@

\end{playground}

Nesting of factors results in the computation of additional error terms, with different degrees of freedom. Whether nesting exists or not is a property of an experiment. It is decided as part of the design of the experiment based on the mechanics of treatment assignment to experimental units. In base R formulas the nesting needs to be accounted by explicit definition of error terms by means of \code{Error()} within the formula. The numerical example below is just a demonstration of the consequences of using an invalid design. Of the two model formulas used, only one yields a valid data analysis!

<<mformulas-05,eval=FALSE>>=
y ~ x1 + x2 + Error(x1 * x2)
@

Packages \pkgname{nlme} and \pkgname{lme4} use their own extensions to base R's model formula syntax to allow the description of nesting and distinguishing fixed and random effects. Additive models have required other extensions, most of them specific to individual packages. These fall outside the scope of this book.

\begin{warningbox}
  \Rlang will accept any syntactically correct model formula, even when the results of the fit are not interpretable. It is the responsibility of the user to ensure that models are meaningful. The most common, and dangerous, mistake are missing simple interactions in factorial ANOVA.

  Fitting models like those below to three-way ANOVA should be avoided. In both cases simpler terms are missing, while interaction(s) that include the missing term are included in the model. Such models are not interpretable, as the variation from the missing term(s) ends being ``disguised'' within the remaining terms, distorting their apparent significance and parameter estimates.

  <<mformulas-W01,eval=FALSE>>=
  y ~ A + B + A * C + B * C + A * B
  y ~ A + B + C + A * B * C
  @

  Models such as those below are interpretable, even though ``incomplete'' (not including all possible interactions).
  <<mformulas-W02,eval=FALSE>>=
  y ~ A + B + C + A * C + B * C + A * B
  y ~ A + B + C + A * B
  @

  The full three-way factorial models includes all possible combinations of factors.
  <<mformulas-W03,eval=FALSE>>=
  y ~ A + B + C + A * C + B * C + A * B + A * B * C
  @

\end{warningbox}

As seen in chapter \ref{chap:R:data} almost everything in the \Rlang language is an object that can be stored and manipulated. Model formulas are also objects, objects of class \code{"formula"}.

<<mformulas-W11>=
class(y ~ x)
@

<<mformulas-W12>>=
a <- y ~ x
class(a)
inherits(a, "formula")
@

\begin{warningbox}
No method \Rfunction{is.formula()} is part of the \Rlang language, but instead methods \Rfunction{is.formula()} and \Rfunction{is\_formula()} provided by packages \pkgname{plyr} and \pkgname{rlang}, respectively.

<<mformulas-EB01c>>=
plyr::is.formula(a)
rlang::is_formula(a)
@

However, it would be unwise to add a dependency on either of these packages for such a trivial function. We could directly use \Rlang method \Rfunction{inherits()} as shown above.

To see how these methods have been defined, we can print them, simply by not supplying the ``function-call'' parentheses.

<<mformulas-EB01d>>=
plyr::is.formula
rlang::is_formula
@

It can be seen that they call \Rfunction{inherits()} with \code{"formula"} as its second argument, demonstrating how trivial their definitions are. This servers as an example of how sometimes unnecessary dependencies on packages can creep into our own scripts and packages, as dependency one package does not only adds a dependency on the used package but also on the further packages it directly and indirectly depends on. If needed or desired one can very easily define a \Rfunction{is.formula()} method that adds no external dependencies.
\end{warningbox}

\begin{explainbox}
\textbf{Manipulation of model formulas.} Being this a book about the \Rlang language, it is pertinent to describe how formulas can be manipulated. Formulas as any other \Rlang objects can be saved in variables including lists. Why is this useful? For example if we want to fit several different models to the same data, we can write a \code{for} loop that walks through a list of model formulas. Or we can write a function that accepts one or more formulas as arguments.

The use of \code{for} \emph{loops} for iteration is described in section \ref{sec:script:flow:control} on page \pageref{sec:script:flow:control}. For now, you need only to know that the statement in the body of the loop is executed once for each member of the \code{formulas} list, with \code{formula} taking successively the value of each member of \code{formulas}.
<<mformulas-EB02>>=
my.data <- data.frame(x = 1:10, y = (1:10) / 2 + rnorm(10))
anovas <- list()
formulas <- list(a = y ~ x - 1, b = y ~ x, c = y ~ x + x^2)
for (formula in formulas) {
 anovas <- c(anovas, list(lm(formula, data = my.data)))
 }
str(anovas, max.level = 1)
@

As could be expected a conversion constructor is available with name \code{as.formula}. It becomes useful when formulas are input interactively by the user or read from text files. We can convert a character string into a formula.

<<mformulas-EB03>>=
my.string <- "y ~ x"
lm(as.formula(my.string), data = my.data)
@

As there are many functions available in base \Rlang and through packages for the manipulation of character strings, it is straightforward to build model formulas programmatically as strings. We can use functions like \code{paste()} to assemble a formula as text, and then use \code{as.formula()} to convert it to an object of class \code{formula}, usable for fitting a model.

<<mformulas-EB04>>=
my.string <- paste("y", "x", sep = "~")
lm(as.formula(my.string), data = my.data)
@

For the reverse operation of converting a formula into a string, we have available methods \code{as.character()} and \code{format()}. The first of these methods returns a character vector containing the components of the formula as individual strings, while \code{format()} returns a single character string with the formula formatted for printing.

<<mformulas-EB05>>=
formatted.string <- format(y ~ x)
formatted.string
as.formula(formatted.string)
@

It is also possible to \emph{edit} formula objects with method \Rfunction{update()}. In the replacement formula, a dot can replace either the left hand side (lhs) or the right hand side (rhs) of the existing formula in the replacement formula. We can also remove terms as can be seen below. In some cases the dot corresponding to the lhs con be omitted, but including it makes the syntax clearer.

<<mformulas-EB06>>=
my.formula <- y ~ x1 + x2
update(my.formula, . ~ . + x3)
update(my.formula, . ~ . - x1)
update(my.formula, . ~ x3)
update(my.formula, z ~ .)
update(my.formula, . + z ~ .)
@

R provides high level functions for model selection. Consequently many R users will rarely need to edit model formulas in their scripts. For example, step wise model selection is possible with \Rlang method \code{step()}.

A matrix of dummy coefficients can be derived from a model formula, a type of contrast and the data for the explanatory variables.
<<mformulas-EB07>>=
treats.df <- data.frame(A = rep(c("yes", "no"), c(4, 4)),
                        B = rep(c("white", "black"), 4))
treats.df
@

The default contrasts types currently in use.
<<mformulas-EB08>>=
options("contrasts")
@

A model matrix for a model for a two-way factorial design with no interaction term.
<<mformulas-EB09>>=
model.matrix(~ A + B, treats.df)
@

A model matrix for a model for a two-way factorial design with interaction term.
<<mformulas-EB10>>=
model.matrix(~ A * B, treats.df)
@

\end{explainbox}

\section{Time series}\label{sec:stat:time:series}

Longitudinal data, when replicated is usually named repeated measurements, but when not replicated, is named time series.
Base \Rlang provides special support for the analysis of time series data, while repeated measurements can be analysed with nested linear models, mixed-effects models and additive models.

Time series data are data collected in such a way that there is only one observation, possibly of multiple variables, available at each time step. This brief section introduces only the most basic aspects of time-series analysis. In most cases time steps are of uniform duration and occur regularly. Given this properties, it is possible to take advantage of this for their storage. \Rlang not only provides methods for the analysis and manipulation of this type data, but also a specialized class for their storage, \code{ts}. A regular time series does not store the time value at each observation: only a combination of two of start time, step size and end time needs to be stored.

We start by creating a time series from a numeric vector. By now, you surely guessed that you need to use a constructor called \Rfunction{ts()} or a conversion constructor called \Rfunction{as.ts()} and that you can look up the arguments they accept by reading the corresponding help pages.

For example for a time series of monthly values we could use.
<<ts-00>>=
my.ts <- ts(1:10, start = 2018, deltat = 1/12)
class(my.ts)
str(my.ts)
@

We next use a data set included in R.
<<ts-01>>=
class(austres)
is.ts(austres)
@

<<include=FALSE, cache=FALSE>>=
opts_chunk$set(opts_fig_wide)
@

This time series of the number of Australian residents is dominated by the increasing trend.
<<ts-02>>=
plot(austres)
@

A different example, in this case meteorological data, shows an important cyclic component. The annual cycle of mean air temperatures is clearly seen in the plot.
<<ts-03>>=
data(nottem)
is.ts(nottem)
plot(nottem)
@

In the next two code chunks two different approaches to time series decomposition are used. In the first one we use a moving average to capture the trend, while in the second approach we use loess for the decomposition, a method for which the acronym STL is used. LOESS is a local-regression-based smoothing method.
<<ts-04>>=
nottem.celcius <- (nottem - 32) * 5/9
plot(decompose(nottem.celcius))
@

<<ts-05>>=
plot(stl(nottem.celcius, s.window = 6))
@

\section{Multivariate statistics}\label{sec:stat:MV}

\subsection{Multivariate analysis of variance}

Multivariate methods take into account several response variables simultaneously, as part of a single analysis. In practice it is usual to use contributed packages for multivariate data analysis in \Rlang, except for simple cases. We will look first at \emph{multivariate} ANOVA or MANOVA. In the same way as \Rfunction{aov()} is a wrapper that uses internally \Rfunction{lm()}, \Rfunction{manova()} is a wrapper that uses internally \Rfunction{aov()}.

Multivariate model formulas in some cases use the same syntax as univariate ones, but contain more than one response variable on their left hand side (lhs).
<<manova-01>>=
y1 + y2 + y3 ~ x1 * x2
@

In other cases, a special syntax is used. In the case of function \Rfunction{manova()} we use function \code{cbind()} (column bind) to assemble the lhs of the formula.
<<manova-02>>=
data(iris)
m.fit <- manova(cbind(Petal.Length, Petal.Width) ~  Species, data = iris)
anova(m.fit)
summary(m.fit)
@

\subsection{Principal components analysis}\label{sec:stat:PCA}

Principal components analysis is used to simplify a data set by combing variables with similar behaviour into ``principal components''. At a later stage, we frequently try to interpret these components in relation to known and/or assumed independent variables. Base \Rlang's function \Rfunction{prcomp()} computes the principal components and accepts additional arguments for centering and scaling.

<<pca-01>>=
pc <- prcomp(iris[c("Sepal.Length", "Sepal.Width",
                    "Petal.Length", "Petal.Width")],
             center = TRUE,
             scale. = TRUE)
@

<<pca-02>>=
pc
@

The rows ``Proportion of Variance'' and ``Cumulative Proportion'' are most informative of the contribution of each principal component (PC) to explaining the variation among observations.
<<pca-03>>=
summary(pc)
@

Method \code{plot()}
<<pca-04>>=
plot(pc)
@

\begin{explainbox}
As for other fitted models, the object returned by function \Rfunction{prcomp()} is a list with multiple components. Further computations and/or formatting can be applied.

<<pca-EB-01>>=
str(pc)
@

\end{explainbox}

<<include=FALSE, cache=FALSE>>=
opts_chunk$set(opts_fig_wide_square)
@

<<pca-05>>=
biplot(pc)
@

Visually more elaborate plots of the principal components and their loadings can be obtained using packages \pkgname{ggplot} described in chapter \ref{chap:R:plotting} starting on page \pageref{chap:R:performance}, package \pkgname{ggfortify} described in section \ref{sec:plot:ggfortify} on page \pageref{sec:plot:ggfortify} and package \pkgname{ggbiplot} described in section \ref{sec:plot:ggbiplot} on page \pageref{sec:plot:ggbiplot}.

\begin{playground}
For growth and morphological data, a log-transformation can be suitable given that variance is frequently proportional to the magnitude of the values measured. We leave as an exercise to repeat the above analysis using transformed values for the dimensions of petals and sepals. How much does the use of transformations change the outcome of the analysis?
\end{playground}

\subsection{Multidimensional scaling}\label{sec:stat:MDS}

In multidimensional scaling we start with a matrix of distances among observations that will need to be calculated from the raw data. We use for the example distances between geographic locations in Europe.

<<mds-01>>=
loc <- cmdscale(eurodist)
@

We can see that the returned object \code{loc} is a \code{matrix}, with names for one of the dimensions.
<<mds-02>>=
class(loc)
head(loc)
@

Two make the code easier to read two vectors are first extracted from the matrix and named \code{x} and \code{y}.
<<mds-03>>=
x <- loc[, 1]
y <- -loc[, 2] # reflect so North is at the top
## note asp = 1, to ensure Euclidean distances are represented correctly
plot(x, y, type = "n", xlab = "", ylab = "", asp = 1, axes = FALSE,
     main = "cmdscale(eurodist)")
text(x, y, rownames(loc), cex = 0.6)
@

\subsection{Clustering}\label{sec:stat:cluster}

In clustering analysis the aim is to group observations into discrete groups with maximal internal homogeneity and maximum group to group differences. Examples of functions in \Rlang and \Rfunction{hclust()} and \Rfunction{kmeans()}.

\subsection{Discrimination analysis}\label{sec:stat:DA}

In discrimination analysis the categories or groups to which objects belong are known \emph{a priori}. The aim is to fit/bild a classifier to will allow us to assign future observations to the different non-overlapping groups.

\subsection{What next?}

Other books are available with detailed descriptions of how to do various types of analyses in \Rlang, included thorough descriptions of the methods briefly presented in this chapter and many other methods not mentioned here. Good examples of books with broad scope that can serve as references for the application of various statistical methods are \citebooktitle{Everitt2009} \autocite{Everitt2011} and \citebooktitle{Crawley2012} \autocite{Crawley2012}, and the classic reference \citebooktitle{Venables2002} \autocite{Venables2002}. Many subject specific books are also available, and some suggestions given at the end of the book on page \pageref{chap:R:readings}.

<<eval=eval_diag, include=eval_diag, echo=eval_diag, cache=FALSE>>=
knitter_diag()
R_diag()
other_diag()
@
