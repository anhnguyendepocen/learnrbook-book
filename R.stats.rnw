% !Rnw root = appendix.main.Rnw
<<echo=FALSE, include=FALSE>>=
opts_chunk$set(opts_fig_wide)
opts_knit$set(concordance=TRUE)
opts_knit$set(unnamed.chunk.label = 'functions-chunk')
@

\chapter{The R language: statistics}\label{chap:R:functions}

\begin{VF}

\VA{NN}{TT}
\end{VF}

\section{Aims of this chapter}

\section{Fitting linear models}
\index{models!linear}
\index{linear models}
\index{LM|see{linear models}}
One important thing to remember is that model `formulas' are used in different contexts: plotting, fitting of models, and tests like $t$-test. The basic syntax is rather consistently followed, although there are some exceptions.

\subsection{Regression}
\index{linear regression}
The R function \Rfunction{lm()} is used next to fit linear models. If the explanatory variable is continuous, the fit is a regression. In the example below, \code{speed} is a numeric variable (floating point in this case). In the ANOVA table calculated for the model fit, in this case a linear regression, we can see that the term for \code{speed} has only one degree of freedom (df) for the denominator.

We first fit the model and save the output as \code{fm1} (A name I invented to remind myself that this is the first fitted-model in this chapter.%
\label{xmpl:fun:lm:fm1}

<<models-1>>=
fm1 <- lm(dist ~ speed, data=cars)
@

The next step is diagnosis of the fit. Are assumptions of the linear model procedure used reasonably fulfilled? In R it is most common to use plots to this end. We show here only one of the four plots normally produced. This quantile vs.\ quantile plot allows to assess how much the residuals deviate from being normally distributed.

<<models-1a>>=
plot(fm1, which = 2)
@

In the case of a regression, calling \Rfunction{summary()} with the fitted model object as argument is most useful as it provides a table of coefficient estimates and their errors. \Rfunction{anova()} applied to the same fitted object, returns the ANOVA table.

<<models-1b>>=
summary(fm1) # we inspect the results from the fit
anova(fm1) # we calculate an ANOVA
@

Let's look at each argument separately: \verb|dist ~ speed| is the specification of the model to be fitted. The intercept is always implicitly included. To `remove' this implicit intercept from the earlier model we can use \verb|dist ~ speed - 1|. In what follows we fit a straight line through the origin ($x = 0$, $y = 0$).

<<models-2>>=
fm2 <- lm(dist ~ speed - 1, data=cars)
plot(fm2, which = 2)
summary(fm2)
anova(fm2)
@

We now we fit a second degree polynomial.

<<models-3>>=
fm3 <- lm(dist ~ speed + I(speed^2), data = cars) # we fit a model, and then save the result
plot(fm3, which = 3) # we produce diagnosis plots
summary(fm3) # we inspect the results from the fit
anova(fm3) # we calculate an ANOVA
@

The ``same'' fit using an orthogonal polynomial. Higher degrees can be obtained by supplying as second argument to \code{poly()} a different positive integer value.

<<models-3a>>=
fm3a <- lm(dist ~ poly(speed, 2), data=cars) # we fit a model, and then save the result
plot(fm3a, which = 3) # we produce diagnosis plots
summary(fm3a) # we inspect the results from the fit
anova(fm3a) # we calculate an ANOVA
@

We can also compare two models, to test whether one of models describes the data better than the other.

<<models-4>>=
anova(fm2, fm1)
@

Or three or more models. But be careful, as the order of the arguments matters.

<<models-5>>=
anova(fm2, fm1, fm3, fm3a)
@

We can use different criteria to choose the best model: significance based on $P$-values or information criteria (AIC, BIC). AIC and BIC penalize the resulting `goodness' based on the number of parameters in the fitted model. In the case of AIC and BIC, a smaller value is better, and values returned can be either positive or negative, in which case more negative is better.

<<>>=
BIC(fm2, fm1, fm3, fm3a)
AIC(fm2, fm1, fm3, fm3a)
@

One can see above that these three criteria not necessarily agree on which is the model to be chosen.

\begin{description}
\item[anova] \code{fm1}
\item[BIC] \code{fm1}
\item[AIC] \code{fm3}
\end{description}

\subsection{Analysis of variance, ANOVA}\label{sec:anova}
\index{analysis of variance}
\index{ANOVA|see{analysis of variance}}
We use as the \code{InsectSpray} data set, giving insect counts in plots sprayed with different insecticides. In these data \code{spray} is a factor with six levels.%
\label{xmpl:fun:lm:fm4}

<<models-6>>=
fm4 <- lm(count ~ spray, data = InsectSprays)
@

<<model-6a>>=
plot(fm4, which = 2)
@

<<model-6b>>=
anova(fm4)
@

\subsection{Analysis of covariance, ANCOVA}
\index{analysis of covariance}
\index{ANCOVA|see{analysis of covariance}}

When a linear model includes both explanatory factors and continuous explanatory variables, we say that analysis of covariance (ANCOVA) is used. The formula syntax is the same for all linear models, what determines the type of analysis is the nature of the explanatory variable(s). Conceptually a factor (an unordered categorical variable) is very different from a continuous variable.

\section{Generalized linear models}
\index{generalized linear models}
\index{GLM|see{generalized linear models}}

Linear models make the assumption of normally distributed residuals. Generalized linear models, fitted with function \Rfunction{glm()} are more flexible, and allow the assumed distribution to be selected as well as the link function.
For the analysis of the \code{InsectSpray} data set, above (section \ref{sec:anova} on page \pageref{sec:anova}) the Normal distribution is not a good approximation as count data deviates from it. This was visible in the quantile--quantile plot above.

For count data GLMs provide a better alternative. In the example below we fit the same model as above, but we assume a quasi-Poisson distribution instead of the Normal.

<<model-10>>=
fm10 <- glm(count ~ spray, data = InsectSprays, family = quasipoisson)
plot(fm10, which = 2)
anova(fm10, test = "F")
@

<<eval=eval_diag, include=eval_diag, echo=eval_diag, cache=FALSE>>=
knitter_diag()
R_diag()
other_diag()
@
