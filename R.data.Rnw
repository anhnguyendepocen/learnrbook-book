% !Rnw root = appendix.main.Rnw

<<echo=FALSE, include=FALSE>>=
opts_chunk$set(opts_fig_wide)
opts_knit$set(concordance=TRUE)
@

\chapter{Storing and manipulating data with R}\label{chap:R:data}

\dictum[Patrick J. Burns (1998) S Poetry. \url{http://www.burns-stat.com/documents/books/s-poetry/}]{Essentially everything in S, for instance, a call to a function, is an S[R] object. One viewpoint is that S[R] has self-knowledge. This self-awareness makes a lot of things possible in S[R] that are not in other languages.}

\section{Packages used in this chapter}

For executing the examples listed in this chapter you need first to load the following packages from the library:

<<message=FALSE>>=
library(tibble)
library(readr)
library(readxl)
library(xlsx)
library(foreign)
library(haven)
library(xml2)
library(RNetCDF)
library(ncdf4)
library(lubridate)
library(jsonlite)
@

\begin{infobox}
The data sets used in this chapter are at the moment avaialble for download. The details of how to download files from within R is explained in section \ref{sec:files:remote} on page \pageref{sec:files:remote}. The examples for local data use the same files. As it is easier to first examplify reading local files, please, run the code in the chunk below at least once, before attempting to run the code in the next sections. Make sure that the current folder/directory is the same one that will be current when running the examples.

The code chunk below will create a folder called \code{data} unless it already exists and download all files except one from my web server. Existing files with the same names will not be overwritten.

<<load-data-00,eval=FALSE>>=
dir.name = "./data"
if (!dir.exists(dir.name)) {
  dir.create(dir.name)
}
# download file in text mode
file.name <- paste(dir.name, "logger_1.txt", sep ="/")
if (!file.exists(file.name)) {
  download.file("http://r4photobiology.info/learnr/logger_1.txt",
                file.name)
}
# download remaining files in binary mode
bin.file.names <- c("my-data.xlsx", "Book1.xlsx", "BIRCH1.SYS",
                    "thiamin.sav", "my-data.sav", "meteo-data.nc")
for (file.name in bin.file.names) {
  f <- paste(dir.name, file.name, sep ="/")
  if (!file.exists(f)) {
    download.file(paste("http://r4photobiology.info/learnr",
                        file.name, sep="/"),
                  f,
                  mode = "wb")
  }
}
# download NetCDF file from NOAA server
file.name <- paste(dir.name, "pevpr.sfc.mon.ltm.nc", sep ="/")
if (!file.exists(file.name)) {
  my.url <- paste("ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis.derived/",
                  "surface_gauss/pevpr.sfc.mon.ltm.nc",
                  sep = "")
  download.file(my.url,
                mode = "wb",
                destfile = paste(dir.name, "pevpr.sfc.mon.ltm.nc", sep ="/"))
}
@

\end{infobox}

\section{Introduction}

By reading previous chapters, you have already become familiar with base R's classes, methods, functions and operators for storing and manipulating data. Several recently developed packages provide somehow different, and in my view easier, ways of working with data in R without compromising performance to a level that would matter outside the realm of `big data'. Some other recent packages emphasize computation speed, at some cost with respect to simplicity of use, and in particular intuitiveness. Of course, as with any user interface, much depends on one's own preferences and attitudes to data analysis. However, a package designed for maximum efficiency like \pkgname{data.table} requires of the user to have a good understanding of computers to be able to understand the compromises and the unusual behavior compared to the rest of R. I will base this chapters on what I mostly use myself for everyday data analysis and scripting, and exclude the complexities of R programming and package development.

The chapter is divided in three sections, the first one deals with reading data from files produced by other programs or instruments, or typed by users outside of R, and querying databases and very briefly on reading data from the internet. The second section will deal with transformations of the data that do not combine different observations, although they may combine different variables from a single observation event, or select certain variables or observations from a larger set. The third section will deal with operations that produce summaries or involve other operations on groups of observations.

\section{Data input and output}

In recent several packages have made it easier and faster to import data into R. This together with wider and faster internet access to data sources, has made it possible to efficiently work with relatively large data sets. The way R is implemented, keeping all data in memory (RAM), imposes limits the size of data sets that can analysed with base R. One option is to use a 64 bit version of R on a computer running a 64 bit operating system. This allows the use of large amounts of RAM if available. For larger data sets, one can use different packages that allow selective reading of data from files, and using queries to obtain subsets of data from databases. We will start with the simplest case, files using the native formats of R itself.

\subsection{.Rda files}\label{sec:data:rda}

In addition to saving the whole workspace, one can save any R object present in the workspace to disk. One or more objects, belonging to any mode or class can be saved into the same file. Reading the file restores all the saved objects into the current workspace. These files are portable across most R versions. Whether compression is used, and whether the files is encoded in ASCII characters---allowing maximum portability at the expense of increased size or not.

We create and save a data frame object.

<<rda-01>>=
my.df <- data.frame(x = 1:10, y = 10:1)
my.df
save(my.df, file = "my-df.rda")
@

We delete the data frame object and confirm that it is no longer present in the workspaceh.
<<rda-02>>=
rm(my.df)
ls(pattern = "my.df")
@

We read the file we earlier saved to restore the object.
<<rda-03>>=
load(file = "my-df.rda")
ls(pattern = "my.df")
my.df
@

The default format used is binary and compressed, which results in smaller files.

\begin{playground}
In the example above, only one object was saved, but one can simply give the names of additional objects as arguments. Just try saving, more than one data frame to the same file. Then the data frames plus a few vectors. Then define a simple function and save it. After saving each file, clear the workspace and then load the objects you save from the file.
\end{playground}

Sometimes it is easier to supply the names of the objects to be saved as a vector of character strings through an argument to parameter \code{list}. One case is when wnating to save a group of objects based on their names. We can use \code{ls()} to list the names of objects matching a simple \code{pattern} or a complex regular expression. The example below does this in two steps saving the character vector first, and then using this saved object as argument to \code{save}'s \code{list} parameter.

<<rda-04>>=
objcts <- ls(pattern = "*.df")
save(list = objcts, file = "my-df1.rda")
@

The intermediate step can be skipped.
<<rda-05>>=
save(list = ls(pattern = "*.df"), file = "my-df1.rda")
@

\begin{playground}
Practice using different patterns with \code{ls()}. You do not need to save the objects to a file. Just have a look at the list of object names returned.
\end{playground}

As a coda, we show how to cleanup by deleting the two files we created. Function \code{unlink()} can also be used to delete folders.
<<rda-06>>=
unlink(c("my-df.rda", "my-df1.rda"))
@

\subsection{File names and portability}\label{sec:files:filenames}

When saving data to files from scripts or code that one expects to be run on a different operating system (OS), we need to be careful to chose files names valid under all OSs where the file could be used. This is specially important when developing R packages. Best avoid space characters as part of file names and the use of more than one dot. For widest portability, underscores should be avoided, while dashes are usually not a problem.

R provides some functions which help with portability, by hiding the idiosyncracies of the different OSs from R code. Different OSs use different characters in paths, for example, and consequently the algorithm needed to extract a file name from a file path, is OS specific. However, R's function \code{basename()} allows the inclusion of this operation in user's code portably.

Under \pgrmname{MS-Windows} paths include backslash characters which are not ``normal'' characters in R, and many other languages, but rather ``escape'' characters. Within R forward slash can be used in their place,

<<filenames-01>>=
basename("C:/Users/aphalo/Documents/my-file.txt")
@

or backslash characters can be ``escaped'' by repeating them.
<<filenames-02>>=
basename("C:\\Users\\aphalo\\Documents\\my-file.txt")
@

The complementary function is \code{dirname()} which extracts the bare path to the containing disk folder, from a full file path.
<<filenames-03>>=
dirname("C:/Users/aphalo/Documents/my-file.txt")
@

\begin{warningbox}
We here use in examples paths and filenames valid in MS-Windows. We have tried to avoid names incompatible with other operating systems, but special characters separating directories (= folders) in paths are different among operating systems. For example, if you use UNIX (e.g.\ AppleÂ´s OS X) or a Linux distribution (such as Debian or Ubuntu) only forward slashes will be recognized as separators.
\end{warningbox}

Functions \code{getwd()} and \code{setwd()} can be used to get the path to the current working directory and to set a directory as current, respectively.

<<filenames-05>>=
getwd()
@

Function \code{setwd} returns the path of the previous working directory, allowing us to portably set the working directory to the previous one. Both relative paths, as in the example, or absolute paths are accepted as arguments.
<<filenames-06>>=
oldwd <- setwd("..")
getwd()
@

The returned value is always an absolute full path, so it remains valid even if the path to the working directory changes more than once before it being restored.
<<filenames-07>>=
oldwd
setwd(oldwd)
getwd()
@

We can also obtain a list of files and/or directories (= disk folders).
<<filenames-09>>=
head(list.files("."))
head(list.dirs("."))
head(dir("."))
@

\begin{playground}
Above we passed \code{"."} as argument for parameter \code{path}. This is the same as the default. Convince yourself that this is indeed the default by calling the functions without an explicit argument. After this, play with the functions trying other existing and non-existent paths in your computer.
\end{playground}

\begin{playground}
Combine the use of \code{basename()} with \code{list.files()} to obtain a list of files names.
\end{playground}

\begin{playground}
Compare the behaviour of functions \code{dir} and \code{lis.dirs()}, and try by overriding the default arguments of \code{list.dirs()}, to get the call to return the same output as \code{dir()} does by default.
\end{playground}

Base R provides several functions for working with files, they are listed in the help page for \code{files} and in individual help pages. Use \code{help("files")} to access the help for the ``family'' of functions.

<<filenames-08>>=
if (!file.exists("xxx.txt")) {
  file.create("xxx.txt")
}
file.size("xxx.txt")
file.info("xxx.txt")
file.rename("xxx.txt", "zzz.txt")
file.exists("xxx.txt")
file.exists("zzz.txt")
file.remove("zzz.txt")
@

\begin{playground}
Function \code{file.path()} can be used to construct a file path from its components in a way that is portable across OSs. Look at the help page and play with the function to assemble some paths that exist in the computer you are using.
\end{playground}

\subsection{Text files}\label{sec:files:txt}

\subsubsection[Base R and `utils']{Base R and \pkgname{utils}}

Text files come many different sizes and formats, but can be divided into two broad groups. Those with fixed format fields, and those with delimited fields. Fixed format fields were especially common in the early days of FORTRAN and COBOL, and computers with very limited resources. They are usually capable of encoding information using fewer characters than with delimited fields. The best way of understanding the differences is with examples. We first discuss base R functions and starting from page \pageref{sec:files:readr} we discuss the functions defined in package \pkgname{readr}.

In a format with delimited fields, a delimiter, in this case ``,'' is used to separate the values to be read. In this example, the values are aligned by inserting ``white space''. This is what is called comma-separated-values format (CSV). Function \code{write.csv()} and \code{read.csv()} can be used to write and read these files using the conventions used in this example.
\begin{verbatim}
 1.0, 24.5, 346, ABC
23.4, 45.6,  78, ZXY
\end{verbatim}

When reading a CSV file, white space is ignored and fields recognized based on separators. In most cases decimal points and exponential notation are allowed for floating point values. Alignment is optional, and helps only reading by humans, as white space is ignored. This miss-aligned version of the example above can be expected to be readable with base R function \code{read.csv()}.
\begin{verbatim}
1.0,24.5,346,ABC
23.4,45.6,78,ZXY
\end{verbatim}

With a fixed format for fields no delimiters are needed, but a description of the format is required. Decoding is based solely on the position of the characters in the line or record. A file like this cannot be interpreted without a description of the format used for saving the data. Files containing data stored in fixed format with fields can be read with base R function \code{read.fwf()}. Records, can be stored in multiple lines, each line with fields of different but fixed widths.
\begin{verbatim}
 10245346ABC
234456 78ZXY
\end{verbatim}

Function \code{read.fortran()} is a wrapper on \code{read.fwf()} that accepts format definitions similar to those used in FORTRAN, but not completely compatible with them. One particularity of FORTRAN \emph{formated data transfer} is that the decimal marker can be omitted in the saved file and its position specified as part of the format definition. Again an additional trick used to make text files (or stacks of punch cards) smaller.

R functions \code{write.table()} and \code{read.table()} default to separating fields with whitespace. Functions \code{write.csv()} and \code{read.csv()} have defaults for their arguments suitable for writing and reading CSV files in English-language locales. Functions \code{write.csv2()} and \code{read.csv2()} are similar have defaults for delimiters and decimal markers suitable for CSV files in locales with languages like Spanish, French, or Finnish that use comma (,) as decimal marker and semi-colon (;) as field delimiter. Another frequently used field delimiter is the ``tab'' or tabulator character, and sometimes any white space character (tab, space). In most cases the records (observations) are delimited by new lines, but this is not the only possible approach as the user can pass the delimiters to used as arguments in the function call.

We give examples of the use of all the functions described in the paragraphs above, starting by writing data to a file, and then reading this file back into the workspace. The \code{write} functions take as argument data frames or objects that can be coerced into data frames. In contrast to \code{save()}, these functions can only write to files data that is in a tabular or matrix-like arrangement.

<<file-io-txt-01>>=
my1.df <- data.frame(x = 1:5, y = 5:1 / 10)
@

We write a CSV file suitable for an English language locale, and then display its contents. In most cases setting \code{row.names = FALSE} when writing a CSV file will help when it is read. Of course, if row names do contain important information, such as gene tags, you cannot skip writing the row names to the file unless you first copy these data into a column in the data frame. (Row names are stored separately as an attribute in \code{data.frame} objects.
<<file-io-txt-02>>=
write.csv(my.df, file = "my-file1.csv", row.names = FALSE)
file.show("my-file1.csv", pager = "console")
@

<<file-io-txt-02a, comment='', echo=FALSE>>=
cat(readLines('my-file1.csv'), sep = '\n')
@

If we had written the file using default settings, reading the file so as to recover the original objects, would have required overriding of the default argument for parameter \code{row.names}.
<<file-io-txt-02b>>=
my_read1.df <- read.csv(file = "my-file1.csv")
my_read1.df
all.equal(my.df, my_read1.df, check.attributes = FALSE)
@

\begin{playground}
Read the file with function \code{read.csv2()} instead of \code{read.csv()}. Although this may look as a waste of time, the point of the exercise is for you to get familiar with R's behaviour in case of such a mistake. This will help you recognize similar errors when they happen accidentally.
\end{playground}

We write a CSV file suitable for a Spanish, Finnish or similar locale, and then display its contents. It can be seen, that the same data frame is saved using different delimiters.
<<file-io-txt-03>>=
write.csv2(my.df, file = "my-file2.csv", row.names = FALSE)
file.show("my-file2.csv", pager = "console")
@

<<file-io-txt-03a, comment='', echo=FALSE>>=
cat(readLines('my-file2.csv'), sep = '\n')
@

As with \code{read.csv()} had we written row names to the file, we would have needed to override the default behaviour.
<<file-io-txt-03b>>=
my_read2.df <- read.csv2(file = "my-file2.csv")
my_read2.df
all.equal(my.df, my_read2.df, check.attributes = FALSE)
@

\begin{playground}
Read the file with function \code{read.csv()} instead of \code{read.csv2()}. This may look as an even more futile exercise than the previous one, but it isn't as the behaviour of R is different. Consider \emph{how} values are erroneously decoded in both exercises. If the \emph{structure} of the data frames read is not clear to you, do use function \code{str()} to look at them.
\end{playground}

We write a file with the fields separated by white space with function \code{write.table()}.
<<file-io-txt-04>>=
write.table(my.df, file = "my-file3.txt", row.names = FALSE)
file.show("my-file3.txt", pager = "console")
@

<<file-io-txt-04a, comment='', echo=FALSE>>=
cat(readLines('my-file3.txt'), sep = '\n')
@

In the case of \code{read.table()} there is no need to override the default, independently of row names are written to the file or not. The reason is related to the default behaviour of the \code{write} functions. Whether they write a column name (\code{""}, an empty character string) or not for the first column, containing the row names.
<<file-io-txt-04b>>=
my_read3.df <- read.table(file = "my-file3.txt", header = TRUE)
my_read3.df
all.equal(my.df, my_read3.df, check.attributes = FALSE)
@

\begin{playground}
If you are still unclear about why the files were decoded in the way they were, now try to read them with \code{read.table()}. Do now the three examples make sense to you?
\end{playground}

Function \code{cat()} takes R objects and writes them after conversion to character strings to a file, inserting one or more characters as separators, by default a space. This separator can be set by an argument through \code{sep}. In our example we set \code{sep} to a new line (entered as the escape sequence \code{"\\n"}.

<<file-io-txt-05>>=
my.lines <- c("abcd", "hello world", "123.45")
cat(my.lines, file = "my-file4.txt", sep = "\n")
file.show("my-file4.txt", pager = "console")
@

<<file-io-txt-05a, comment='', echo=FALSE>>=
cat(readLines('my-file4.txt'), sep = '\n')
@

<<file-io-txt-05b>>=
my_read.lines <- readLines('my-file4.txt')
my_read.lines
all.equal(my.lines, my_read.lines, check.attributes = FALSE)
@

\begin{warningbox}
There are couple of things to take into account when reading data from text files using base R functions \code{read.table()} and its relatives: by default columns containing character strings are converted into factors, and column names are sanitised (spaces and other ``inconvenient'' characters replaced with dots).
\end{warningbox}

\subsubsection[readr]{\pkgname{readr}}\label{sec:files:readr}

<<>>=
citation(package = "readr")
@

Package \pkgname{readr} is part of the \pkgname{tidyverse} suite. It defines functions that allow much faster input and output, and have different default behaviour. Contrary to base R functions, they are optimized for speed, but may sometimes wrongly decode their input and sometimes silently do this even for some CSV files that are correctly decoded by the base functions. Base R functions are dumb, the file format or delimiters must be supplied as arguments. The \pkgname{readr} functions use ``magic'' to guess the format, in most cases they succeed, which is very handy, but occasionally the power of the magic is not strong enough. The ``magic'' can be overridden by passing arguments. Another important advantage is that these functions read character strings formatted as dates or times directly into columns of class \code{datetime}.

All \code{write} functions defined in this package have an \code{append} parameter, which can be used to change the default behaviour of overwriting an existing file with the same name, to appending the output at its end.

Although in this section we exemplify the use of these functions by passing a file name as argument, URLs, and open file descriptors are also accepted. Furthermore, if the file name ends in a tag recognizable as indicating a compressed file format, the file will be uncompressed on-the-fly.

\begin{warningbox}
The names of functions ``equivalent'' to those described in the previous section have names formed by replacing the dot with an underscore, e.g.\ \code{read\_csv()} $\approx$ \code{read.csv()}. The similarity refers to the format of the files read, but not the order, names or roles of their formal parameters. Function \code{read\_table()} has a different behaviour to \code{read.table()}, although they both read fields separated by white space, \code{read\_table()} expects the fields in successive records (usually lines) to be vertically aligned while \code{read.table()} tolerates vertical misalignment. Other aspects of the default behaviour are also different, for example these functions do not convert columns of character strings into factors and row names are not set in the returned data frame (truly a \code{tibble} which inherits from \code{data.frame}).
\end{warningbox}

<<readr-01>>=
read_csv(file = "my-file1.csv")
@

<<readr-02>>=
read_csv2(file = "my-file2.csv")
@

Because of the vertically misaligned fields in file \code{my-file3.txt}, we need to use \code{read\_delim()} instead of \code{read\_table()}.
<<readr-03>>=
read_delim(file = "my-file3.txt", " ")
@

\begin{playground}
See what happens when you modify the code to use \code{read} functions to read files that are not matched to them---i.e.\ mix and match functions and files from the three code chunks above. As mentioned earlier forcing errors will help you learn how to diagnose when such errors are caused by coding mistakes.
\end{playground}

We demonstrate here the use of \code{write\_tsv()} to produce a text file with tab-separated fields.
<<readr-04>>=
write_tsv(my.df, path = "my-file5.tsv")
file.show("my-file5.tsv", pager = "console")
@

<<readr-04a, comment='', echo=FALSE>>=
cat(readLines('my-file5.tsv'), sep = '\n')
@

<<readr-04b>>=
my_read4.df <- read_tsv(file = "my-file5.tsv")
my_read4.df
all.equal(my.df, my_read4.df, check.attributes = FALSE)
@

We demonstrate here the use of \code{write\_excel\_csv()} to produce a text file with comma-separated fields suitable for reading with Excel.
<<readr-05>>=
write_excel_csv(my.df, path = "my-file6.csv")
file.show("my-file6.csv", pager = "console")
@

<<readr-05a, comment='', echo=FALSE>>=
cat(readLines('my-file6.csv'), sep = '\n')
@

\begin{playground}
Compare the output from \code{write\_excel\_csv()} and \code{write\_csv()}. What is the difference? Does it matter when you import the written CSV file into Excel (the version you are using, with the locale settings of your computer).
\end{playground}

<<readr-06>>=
write_lines(my.lines, path = "my-file7.txt")
file.show("my-file7.txt", pager = "console")
@

<<readr-06a, comment='', echo=FALSE>>=
cat(read_lines("my-file7.txt"), sep = '\n')
@

<<readr-06b>>=
my_read.lines <- read_lines("my-file7.txt")
my_read.lines
all.equal(my.lines, my_read.lines, check.attributes = FALSE)
@

Additional write and read functions not mentioned are also provided by the package: \code{write\_csv()}, \code{write\_delim()}, \code{write\_file()}, and \code{read\_fwf()}.

\begin{advplayground}
Use \code{write\_file()} to write a file that can be read with \code{read\_csv()}.
\end{advplayground}

\subsection{Worksheets}\label{sec:files:worksheets}

Microsoft Office, Open Office and Libre Office are the most frequently used suites containing programs based on the worksheet paradigm. There is available a standardized file format for exchange of worksheet data, but it does not support all the features present in native file formats. We will start by considering MS-Excel. The file format used by Excel has changed significantly over the years, and old formats tend to be less well supported by available R packages and may require the file to be updated to a more modern format with Excel itself before import into R. The current format is based on XML and relatively simple to decode, older binary formats are more difficult. Consequently for the format currently in use, there are alternatives.

\subsubsection{Exporting CSV files}

If you have access to the original software used, then exporting a worksheet to a text file in CSV format and importing it into R using the functions described in section \ref{sec:files:txt} starting on page \pageref{sec:files:txt} is a workable solution. It is not ideal from the perspective of storing the same data set repeatedly, which, can lead to these versions diverging when updated. A better approach is to, when feasible, to import the data directly from the workbook or worksheets into R.

\subsubsection['readxl']{\pkgname{readxl}}\label{sec:files:excel}

<<readxl-00>>=
citation(package = "readxl")
@

This package exports only two functions for reading Excel workbooks in xlsx format. The interface is simple, and the package easy to instal. We will import a file that in Excel looks as in the screen capture below.

\begin{center}
\includegraphics[width=0.75\textwidth]{data/Book1-xlsx.png}
\end{center}

We first list the sheets contained in the workbook file.
<<readxl-01>>=
sheets <- excel_sheets("data/Book1.xlsx")
sheets
@

In this case the argument passed to \code{sheet} is redundant, as there is only a single worksheet in the file. It is possible to use either the name of the sheet or a positional index (in this case \code{1} would be equivalent to \code{"my data"}).
<<readxl-02>>=
Book1.df <- read_excel("data/Book1.xlsx", sheet = "my data")
Book1.df
@

Of the remaining arguments, \code{skip} is useful when we need to skip the top row of a worksheet.

\subsubsection['xlsx']{\pkgname{xlsx}}

Package \pkgname{xlsx} can be more difficult to install as it uses Java functions to do the actual work. However, it is more comprehensive, with functions both for reading and writing Excel worksheet and workbooks, in different formats. It also allows selecting regions of a worksheet to be imported.

<<xlsx-00>>=
citation(package = "xlsx")
@

<<xlsx-01>>=
Book1_xlsx.df <- read.xlsx("data/Book1.xlsx", sheetName = "my data")
Book1_xlsx.df
@

<<xlsx-02>>=
Book1_xlsx2.df <- read.xlsx2("data/Book1.xlsx", sheetIndex = 1)
Book1_xlsx2.df
@

With the three different functions we get a data frame or a tibble, which is compatible with data frames.
<<xlsx-03>>=
class(Book1.df)
class(Book1_xlsx.df)
class(Book1_xlsx2.df)
@

However, the columns are imported differently. Both \code{Book1.df} and \code{Book1\_xlsx.df} differ only in that the second column, a character variable, has been converted into a factor or not. This is to be expected as packages in the \pkgname{tidyverse} suite default to preserving character variables as such, while base R functions convert them to factors. The third function, \code{read.xlsx2()}, did not decode numeric values correctly, and converted everything into factors. This function is reported as being much faster than \code{read.xlsx()}.
<<xlsx-04>>=
sapply(Book1.df, class)
sapply(Book1_xlsx.df, class)
sapply(Book1_xlsx2.df, class)
@

We can also write data frames out to Excel worksheets and even append new worksheets to an existing workbook.
<<xlsx-05>>=
set.seed(456321)
my.data <- data.frame(x = 1:10, y = 1:10 + rnorm(10))
write.xlsx(my.data, file = "data/my-data.xlsx", sheetName = "first copy")
write.xlsx(my.data, file = "data/my-data.xlsx", sheetName = "second copy", append = TRUE)
@

When opened in Excel we get a workbook, containing two worksheets, named using the arguments we passed through \code{sheetName} in the code chunk above.
\begin{center}
\includegraphics[width=0.75\textwidth]{data/my-data-xlsx.png}
\end{center}

\begin{playground}
If you have some worksheet files available, import them into R, to get a feel of how the way data is organized in the worksheets affects how easy or difficult it is to read the data from them.
\end{playground}

\subsubsection['xml2']{\pkgname{xml2}}

Several modern data exchange formats are based on the XML standard format which uses schema for flexibility. Package \pkgname{xml2} provides functions for reading and parsing such files, as well as HTML files. This is a vast subject, of which I will only give a brief introduction.

We first read a very simple web page.

<<xml2-01>>=
web_page <- read_html("http://r4photobiology.info/R/index.html")
html_structure(web_page)
@

And we extract the text from its \code{title} attribute.

<<xml2-02>>=
xml_text(xml_find_all(web_page, ".//title"))
@

The functions defined in this package and in package \pkgname{XML} can be used to ``harvest'' data from web pages, but also to read data from files using formats that are defined through XML schemas.

\subsection{Statistical software}\label{sec:files:stat}

There are two different comprehensive packages for importing data saved from other statistical such as SAS, Statistica, SPSS, etc. The long time ``standard'' the \pkgname{foreign} package and the much newer \pkgname{haven}. In the case of files saved with old versions of statistical programs, functions from \pkgname{foreign} tend to be more more robust than those from \pkgname{haven}.

\subsubsection[foreign]{\pkgname{foreign}}

Functions in this package allow to import data from files saved by several foreign statistical analysis programs, including \pgrmname{SAS}, \pgrmname{Stata} and \pgrmname{SPPS} among others, and a function for writing data into files with formats native to these three programs. Documentation is included with R describing them in \emph{R Data Import/Export}. We give here just a simple example with a \pgrmname{SPSS} file saved recently,

<<foreign-01>>=
my_spss.df <- read.spss(file = "data/my-data.sav", to.data.frame = TRUE)
head(my_spss.df)
@
with dates not converted into R's datetime objects but into numbers.

A second example, this time with a simple \code{.SAV} file saved 15 years ago.

<<foreign-02>>=
thiamin.df <- read.spss(file = "data/thiamin.sav", to.data.frame = TRUE)
head(thiamin.df)
@

Another example, for a Systat file saved on an PC more than 20 years ago.

<<foreign-03>>=
my_systat.df <- read.systat(file = "data/BIRCH1.SYS")
my_systat.df
@

The functions in \pkgname{foreign} can return data frames, but not always this is the default.

\subsubsection[haven]{\pkgname{haven}}

The recently released package \pkgname{haven} is less ambitious in scope, providing read and write functions for only three file formats: \pgrmname{SAS}, \pgrmname{Stata} and \pgrmname{SPSS}. On the other hand \pkgname{haven} provides flexible ways to convert the different labelled values that cannot be directly mapped to normal R modes. They also decode dates and times according to the idiosyncrasies of each of these file formats. The returned \code{tibble} objects in cases when the imported file contained labelled values needs some further work from the user before obtaining `normal' data-frame-compatible \code{tibble} objects.

We import here a \code{.SAV} file saved by a recent version of \pgrmname{SPSS}.

<<haven-01>>=
my_spss.tb <- read_sav(file = "data/my-data.sav")
my_spss.tb
head(my_spss.tb$harvest_date)
@

In this case the dates are correctly decoded.

And an \pgrmname{SPSS}'s \code{.SAV} file saved 15 years ago.

<<haven-02>>=
thiamin.tb <- read_sav(file = "data/thiamin.sav")
thiamin.tb
thiamin.tb <- as_factor(thiamin.tb)
thiamin.tb
@

\begin{playground}
Compare the values returned by different \code{read} functions when applied to the same file on disk. Use \code{names()}, \code{str()} and \code{class} as tools in your exploration. If you are brave, also use \code{attributes()}, \code{mode()}, \code{dim()}, \code{dimnames()}, \code{nrow()} and \code{ncol()}.
\end{playground}

\begin{playground}
If you use or have used in the past other statistical software or a general purpose language like \langname{Python}, look up some files, and import them into R.
\end{playground}

\subsection{NetCDF files}

In some fields including geophysics and meteorology NetCDF is a very common format for the exchange of data. It is also used in other contexts in which data is referenced to an array of locations, like with data read from Affymetrix micro arrays used to study gene expression. The NetCDF format allows the storage of metadata together with the data itself in a well organized and standardized format, which is ideal for exchange of moderately large data sets.

Officially described as
\begin{quote}
NetCDF is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.
\end{quote}

As sometimes NetCDF files are large, it is good that it is possible to selectively read the data from individual variables with functions in packages \pkgname{ncdf4} or \pkgname{RNetCDF}. On the other hand, this implies that contrary to other data file reading operations, reading a NetCDF file is done in two or more steps.

\subsubsection[ncdf4]{\pkgname{ncdf4}}

We first need to read an index into the file contents, and in additional steps we read a subset of the data. With \code{print} we can find out the names and characteristics of the variables and attributes. In this example we use
long term averages for potential evapotranspiration (PET).

<<ncdf4-01>>=
meteo_data.nc <- nc_open("data/pevpr.sfc.mon.ltm.nc")
# very long output
# print(meteo_data.nc)
@

\begin{playground}
Uncomment the \code{print()} statement above and study the metadata available for the data set as a whole, and for each variable.
\end{playground}
The dimensions of the array data are described with metadata, mapping indexes to in our examples a grid of latitudes and longitudes and a time vector as a third dimension. The dates are returned as character strings.

<<ncdf4-02>>=
time.vec <- ncvar_get(meteo_data.nc, "time")
head(time.vec)
longitude <-  ncvar_get(meteo_data.nc, "lon")
head(longitude)
latitude <- ncvar_get(meteo_data.nc, "lat")
head(latitude)
@

The \code{time} vector is rather odd, as it contains only month data as these are long-term averages. From the metadata we can infer that they correspond to the months of the year, and we directly generate these, instead of attemping a conversion.

We construct a \code{tibble} object with PET values for one grid point, we can take advantage of \emph{recycling} or short vectors.

<<ncdf4-03>>=
pet.tb <-
    tibble(moth = month.abb[1:12],
           lon = longitude[6],
           lat = latitude[2],
           pet = ncvar_get(meteo_data.nc, "pevpr")[6, 2, ]
           )
pet.tb
@

If we want to read in several grid points, we can use several different approaches. In this example we take all latitudes along one longitude. Here we avoid using loops altogether when creating a \emph{tidy} \code{tibble} object. However, because of how the data is stored, we needed to transpose the intermediate array before conversion into a vector.

<<ncdf4-04>>=
pet2.tb <-
    tibble(moth = rep(month.abb[1:12], length(latitude)),
           lon = longitude[6],
           lat = rep(latitude, each = 12),
           pet = as.vector(t(ncvar_get(meteo_data.nc, "pevpr")[6, , ]))
           )
pet2.tb
subset(pet2.tb, lat == latitude[2])
@

\begin{playground}
Play with \code{as.vector(t(ncvar\_get(meteo\_data.nc, "pevpr")[6, , ]))} until you understand what is the effect of each of the nested function calls, starting from \code{ncvar\_get(meteo\_data.nc, "pevpr")}. You will also want to use \code{str()} to see the structure of the objects returned at each stage.
\end{playground}

\begin{playground}
Instead of extracting data for one longitude across latitudes, extract data across longitudes for one latitude near the Equator.
\end{playground}

\subsubsection[RNetCDF]{\pkgname{RNetCDF}}

\begin{warningbox}
Package RNetCDF supports NetCDF3 files, but not those saved using the current NetCDF4 format.
\end{warningbox}

We first need to read an index into the file contents, and in additional steps we read a subset of the data. With \code{print.nc} we can find out the names and characteristics of the variables and attributes.

<<netcdf-01>>=
meteo_data.nc <- open.nc("data/meteo-data.nc")
str(meteo_data.nc)
# very long output
# print.nc(meteo_data.nc)
@

The dimensions of the array data are described with metadata, mapping indexes to in our examples a grid of latitudes and longitudes and a time vector as a third dimension. The dates are returned as character strings.

<<netcdf-02>>=
time.vec <- var.get.nc(meteo_data.nc, "time")
head(time.vec)
longitude <-  var.get.nc(meteo_data.nc, "lon")
head(longitude)
latitude <-  var.get.nc(meteo_data.nc, "lat")
head(latitude)
@

We construct a \code{tibble} object with values for midday UV Index for 26 days. For convenience, we convert the strings into R's datetime objects.

<<netcdf-03>>=
uvi.tb <-
    tibble(date = ymd(time.vec, tz="EET"),
           lon = longitude[6],
           lat = latitude[2],
           uvi = var.get.nc(meteo_data.nc, "UVindex")[6,2,]
           )
uvi.tb
@

\subsection{Remotely located data}\label{sec:files:remote}

Many of the functions described above accept am URL address in place of file name. Consequently files can be read remotely, without a separate step. This can be useful, especially when file names are generated within a script. However, one should avoid, especially in the case of servers open to public access, not to generate unnecessary load on server and/or network traffic by repeatedly downloading the same file. Because of this, our first example reads a small file from my own web site. See section \ref{sec:files:txt} on page \pageref{sec:files:txt} for details of the use of these and other functions for reading text files.

<<url-01>>=
logger.df <-
      read.csv2(file = "http://r4photobiology.info/learnr/logger_1.txt",
                header = FALSE,
                col.names = c("time", "temperature"))
sapply(logger.df, class)
sapply(logger.df, mode)
@

<<url-02>>=
logger.tb <-
    read_csv2(file = "http://r4photobiology.info/learnr/logger_1.txt",
              col_names = c("time", "temperature"))
sapply(logger.tb, class)
sapply(logger.tb, mode)
@

While functions in package \pkgname{readr} support the use of URLs, those in packages \pkgname{readxl} and \pkgname{xlsx} do not. Consequently we need to first download the file writing a file locally, that we can read as described in section \ref{sec:files:excel} on page \pageref{sec:files:excel}.

<<url-11>>=
download.file("http://r4photobiology.info/learnr/my-data.xlsx",
              "data/my-data-dwn.xlsx",
              mode = "wb")
@

Functions in package \pkgname{foreign}, as well as those in package \pkgname{haven} support URLs. See section \ref{sec:files:stat} on page \pageref{sec:files:stat} for more information about importing this kind of data into R.

<<url-03>>=
remote_thiamin.df <-
  read.spss(file = "http://r4photobiology.info/learnr/thiamin.sav",
            to.data.frame = TRUE)
head(remote_thiamin.df)
@

<<url-04>>=
remote_my_spss.tb <-
    read_sav(file = "http://r4photobiology.info/learnr/thiamin.sav")
remote_my_spss.tb
@

Function \code{download.file()} in R's default \pkgname{utils} package can be used to download files using URLs. It supports differemt modes such as binary or text, and write or append, and different methods such as internal, wget and libcurl.

In this example we use a downloaded NetCDF file of long-term means for potential evapotranspiration from NOOA, the same used above in the \pkgname{ncdf4} example. This is a moderately large file at 444~KB. In this case we cannot directly open the connection to the NetCDF file, we first download it (commented out code, as we have a local copy), and then we open the local file.

<<url-05,eval=-2>>=
my.url <- paste("ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis.derived/",
                "surface_gauss/pevpr.sfc.mon.ltm.nc",
                sep = "")
#download.file(my.url,
#              mode = "wb",
#              destfile = "data/pevpr.sfc.mon.ltm.nc")
pet_ltm.nc <- nc_open("data/pevpr.sfc.mon.ltm.nc")
@

\begin{warningbox}
For portability NetCDF files should be downloaded in binary mode, setting \code{mode = "wb"}, which is required at least under MS-Windows.
\end{warningbox}

\subsection{Data acquisition from physical devices}\label{sec:data:acquisition}

Numerous modern data acquisition devices based on microcontrolers, including internet-of-things (IoT) devices, have servers (or daemons) that can be queried over a network connection to retrieve either real-time or looged data. Formats based on XML schemas or in JSON format are commonly used.

\subsubsection[jsonlite]{\pkgname{jsonlite}}

We give here a simple example using a module from the \href{http://www.yoctopuce.com/}{YoctoPuce} family using a software hub running locally. We retrieve logged data from a YoctoMeteo module.

\begin{infobox}
This example is not run, and needs setting the configuration of the YoctoPuce module beforehand. Fully reproducible examples, including configuration instructions, will be included in a future revision of the manuscript.
\end{infobox}

<<iot-01,eval=FALSE>>=
hub.url <- "http://127.0.0.1:4444/"
Meteo01.lst <-
    fromJSON(paste(hub.url, "byName/Meteo01/dataLogger.json",
                   sep = ""))
names(Meteo01.lst)
Meteo01.lst
@

The minimum, mean and maximum values for each logging interval, need to be split from a single vector. We do this by indexing with a logical vector (recycled). The data returned is \emph{tidy} with respect to the variables, with quantity names and units also returned by the module, as well as the time.

<<iot-02,eval=FALSE>>=
    val.vector <- unlist(Meteo01.lst[["val"]])
    dplyr::transmute(Meteo01.lst,
                     utc.time = as.POSIXct(utc, origin = "1970-01-01", tz = "UTC"),
                     qty = qty.name,
                     unit = qty.unit,
                     minimum = val.vector[c(TRUE, FALSE, FALSE)],
                     mean = val.vector[c(FALSE, TRUE, FALSE)],
                     maximum = val.vector[c(FALSE, FALSE, TRUE)],
                     dur,
                     freq)
@

\subsection{Databases}\label{sec:data:db}

One of the advantages of using databases is that subsets of cases and variables can be retrieved from databases, even remotely, making it possible to work both locally and remotely with huge data sets. One should remember thar R natively keeps whole objects in RAM, and consequently available machine memory limits the size of data sets with which it is possible to work.

\begin{infobox}
The contents of the remaining sections of this chapter is not yet decided as there are several good books on these aspects of data analysis with R. I recomend the book \citetitle{Wickham2017} \autocite{Wickham2017} for learning how to use the packages in the \pkgname{tidyverse} suite. The book \citetitle{Peng2017} \autocite{Peng2017} covers these subjects in the first chapters before moving on to programming.
\end{infobox}

\section{Pipes and tees}

\subsection{Processing data step by step}

\subsection{Pipes and tees in the Unix shell}

\subsection{Pipes and tees in R scripts}

\section{Row-wise data manipulations}

\subsection{Computations}

\subsection{Subsetting}

\subsection{Merging and joints}

\section{Column-wise data manipulations}

\subsection{Grouping}

\subsection{Summaries}

\subsection{Variable selection}
