% !Rnw root = appendix.main.Rnw

<<echo=FALSE, include=FALSE>>=
opts_chunk$set(opts_fig_wide)
opts_knit$set(concordance=TRUE)
@

\chapter{Storing and manipulating data with R}\label{chap:R:data}

\dictum[Patrick J. Burns (1998) S Poetry. \url{http://www.burns-stat.com/documents/books/s-poetry/}]{Essentially everything in S[R], for instance, a call to a function, is an S[R] object. One viewpoint is that S[R] has self-knowledge. This self-awareness makes a lot of things possible in S[R] that are not in other languages.}

\section{Aims of this chapter}

Base R includes many functions for importing and or manipulating data. This is a complete set, that supports all the usually needed operations. However, many of these functions have not been designed to perform optimally on very large data sets \autocite[see][]{Matloff2011}. The usual paradigm consists in indexing more complex objects, such as arrays and data frames to apply math operantions on vectors. Quite some effort has been put into improving the implementation of these operations on several fronts, 1) designing an enhanced user interface, that it is simpler to use and also easier to optimize for performance, 2) adding to the existing paradigm of allways copying arguments passed to functions, an additional semantics based on the use of \emph{references} to variables, and 3) allowing reading data into memory selectively from files.

The aim of this chapter is to describe, and show how, some of the existing enhancements available through CRAN, can be useful both with small and large data sets.

\section{Packages used in this chapter}

<<eval=FALSE>>=
install.packages(learnrbook::pkgs_ch_data)
@

For executing the examples listed in this chapter you need first to load the following packages from the library:

<<message=FALSE>>=
library(learnrbook)
library(tibble)
library(magrittr)
library(stringr)
library(dplyr)
library(tidyr)
library(readr)
library(readxl)
library(xlsx)
library(foreign)
library(haven)
library(xml2)
library(RNetCDF)
library(ncdf4)
library(lubridate)
library(jsonlite)
@

\begin{infobox}
Some data sets used in this and other chapters are available in package \pkgname{learnrbook}. In addition to the
R data objects, we provide files saved in \emph{foreign} formats, which we used in examples on how to import data. The files can be either read from the R library, or from a copy in a local folder. In this chapter we
assume the user has copied the folder \code{"extdata"} from the package to his working folder.

Copy the files using:

<<copy-data-files>>=
pkg.path <- system.file("extdata", package = "learnrbook")
file.copy(pkg.path, ".", overwrite = TRUE, recursive = TRUE)
@

We also make sure the folder used to save data read from the internet, exists.

<<make-dir>>=
save.path = "./data"
if (!dir.exists(save.path)) {
  dir.create(save.path)
}
@
\end{infobox}

\section{Introduction}

By reading previous chapters, you have already become familiar with base R's classes, methods, functions and operators for storing and manipulating data. Several recently developed packages provide somehow different, and in my view easier, ways of working with data in R without compromising performance to a level that would matter outside the realm of `big data'. Some other recent packages emphasize computation speed, at some cost with respect to simplicity of use, and in particular intuitiveness. Of course, as with any user interface, much depends on one's own preferences and attitudes to data analysis. However, a package designed for maximum efficiency like \pkgname{data.table} requires of the user to have a good understanding of computers to be able to understand the compromises and the unusual behavior compared to the rest of R. I will base this chapters on what I mostly use myself for everyday data analysis and scripting, and exclude the complexities of R programming and package development.

The chapter is divided in three sections, the first one deals with reading data from files produced by other programs or instruments, or typed by users outside of R, and querying databases and very briefly on reading data from the internet. The second section will deal with transformations of the data that do not combine different observations, although they may combine different variables from a single observation event, or select certain variables or observations from a larger set. The third section will deal with operations that produce summaries or involve other operations on groups of observations.

\section{Data input and output}\label{sec:data:io}

In recent several packages have made it easier and faster to import data into R. This together with wider and faster internet access to data sources, has made it possible to efficiently work with relatively large data sets. The way R is implemented, keeping all data in memory (RAM), imposes limits the size of data sets that can analysed with base R. One option is to use a 64 bit version of R on a computer running a 64 bit operating system. This allows the use of large amounts of RAM if available. For larger data sets, one can use different packages that allow selective reading of data from files, and using queries to obtain subsets of data from databases. We will start with the simplest case, files using the native formats of R itself.

\subsection{.Rda files}\label{sec:data:rda}

In addition to saving the whole workspace, one can save any R object present in the workspace to disk. One or more objects, belonging to any mode or class can be saved into the same file. Reading the file restores all the saved objects into the current workspace. These files are portable across most R versions. Whether compression is used, and whether the files is encoded in ASCII characters---allowing maximum portability at the expense of increased size or not.

We create and save a data frame object.

<<rda-01>>=
my.df <- data.frame(x = 1:10, y = 10:1)
my.df
save(my.df, file = "my-df.rda")
@

We delete the data frame object and confirm that it is no longer present in the workspace.
<<rda-02>>=
rm(my.df)
ls(pattern = "my.df")
@

We read the file we earlier saved to restore the object.
<<rda-03>>=
load(file = "my-df.rda")
ls(pattern = "my.df")
my.df
@

The default format used is binary and compressed, which results in smaller files.

\begin{playground}
In the example above, only one object was saved, but one can simply give the names of additional objects as arguments. Just try saving, more than one data frame to the same file. Then the data frames plus a few vectors. Then define a simple function and save it. After saving each file, clear the workspace and then load the objects you save from the file.
\end{playground}

Sometimes it is easier to supply the names of the objects to be saved as a vector of character strings through an argument to parameter \code{list}. One case is when wanting to save a group of objects based on their names. We can use \Rfunction{ls()} to list the names of objects matching a simple \code{pattern} or a complex regular expression. The example below does this in two steps saving the character vector first, and then using this saved object as argument to \code{save}'s \code{list} parameter.

<<rda-04>>=
objcts <- ls(pattern = "*.df")
save(list = objcts, file = "my-df1.rda")
@

The intermediate step can be skipped.
<<rda-05>>=
save(list = ls(pattern = "*.df"), file = "my-df1.rda")
@

\begin{playground}
Practice using different patterns with \Rfunction{ls()}. You do not need to save the objects to a file. Just have a look at the list of object names returned.
\end{playground}

As a coda, we show how to cleanup by deleting the two files we created. Function \code{unlink()} can also be used to delete folders.
<<rda-06>>=
unlink(c("my-df.rda", "my-df1.rda"))
@

\subsection{File names and portability}\label{sec:files:filenames}

When saving data to files from scripts or code that one expects to be run on a different operating system (OS), we need to be careful to chose files names valid under all OSs where the file could be used. This is specially important when developing R packages. Best avoid space characters as part of file names and the use of more than one dot. For widest portability, underscores should be avoided, while dashes are usually not a problem.

R provides some functions which help with portability, by hiding the idiosyncracies of the different OSs from R code. Different OSs use different characters in paths, for example, and consequently the algorithm needed to extract a file name from a file path, is OS specific. However, R's function \Rfunction{basename()} allows the inclusion of this operation in user's code portably.

Under \pgrmname{MS-Windows} paths include backslash characters which are not ``normal'' characters in R, and many other languages, but rather ``escape'' characters. Within R forward slash can be used in their place,

<<filenames-01>>=
basename("extdata/my-file.txt")
@

or backslash characters can be ``escaped'' by repeating them.
<<filenames-02>>=
basename("extdata\\my-file.txt")
@

The complementary function is \Rfunction{dirname()} which extracts the bare path to the containing disk folder, from a full file path.
<<filenames-03>>=
dirname("extdata/my-file.txt")
@

\begin{warningbox}
We here use in examples paths and filenames valid in MS-Windows. We have tried to avoid names incompatible with other operating systems, but special characters separating directories (= folders) in paths are different among operating systems. For example, if you use UNIX (e.g.\ AppleÂ´s OS X) or a Linux distribution (such as Debian or Ubuntu) only forward slashes will be recognized as separators.
\end{warningbox}

Functions \Rfunction{getwd()} and \Rfunction{setwd()} can be used to get the path to the current working directory and to set a directory as current, respectively.

<<filenames-05>>=
getwd()
@

Function \Rfunction{setwd()} returns the path of the previous working directory, allowing us to portably set the working directory to the previous one. Both relative paths, as in the example, or absolute paths are accepted as arguments.
<<filenames-06>>=
oldwd <- setwd("..")
getwd()
@

The returned value is always an absolute full path, so it remains valid even if the path to the working directory changes more than once before it being restored.
<<filenames-07>>=
oldwd
setwd(oldwd)
getwd()
@

We can also obtain a list of files and/or directories (= disk folders).
<<filenames-09>>=
head(list.files("."))
head(list.dirs("."))
head(dir("."))
@

\begin{playground}
Above we passed \code{"."} as argument for parameter \code{path}. This is the same as the default. Convince yourself that this is indeed the default by calling the functions without an explicit argument. After this, play with the functions trying other existing and non-existent paths in your computer.
\end{playground}

\begin{playground}
Combine the use of \Rfunction{basename()} with \Rfunction{list.files()} to obtain a list of files names.
\end{playground}

\begin{playground}
Compare the behaviour of functions \code{dir} and \Rfunction{lis.dirs()}, and try by overriding the default arguments of \Rfunction{list.dirs()}, to get the call to return the same output as \Rfunction{dir()} does by default.
\end{playground}

Base R provides several functions for working with files, they are listed in the help page for \code{files} and in individual help pages. Use \code{help("files")} to access the help for the ``family'' of functions.

<<filenames-08>>=
if (!file.exists("xxx.txt")) {
  file.create("xxx.txt")
}
file.size("xxx.txt")
file.info("xxx.txt")
file.rename("xxx.txt", "zzz.txt")
file.exists("xxx.txt")
file.exists("zzz.txt")
file.remove("zzz.txt")
@

\begin{playground}
Function \Rfunction{file.path()} can be used to construct a file path from its components in a way that is portable across OSs. Look at the help page and play with the function to assemble some paths that exist in the computer you are using.
\end{playground}

\subsection{Text files}\label{sec:files:txt}

\subsubsection[Base R and `utils']{Base R and \pkgname{utils}}

Text files come many different sizes and formats, but can be divided into two broad groups. Those with fixed format fields, and those with delimited fields. Fixed format fields were especially common in the early days of FORTRAN and COBOL, and computers with very limited resources. They are usually capable of encoding information using fewer characters than with delimited fields. The best way of understanding the differences is with examples. We first discuss base R functions and starting from page \pageref{sec:files:readr} we discuss the functions defined in package \pkgname{readr}.

In a format with delimited fields, a delimiter, in this case ``,'' is used to separate the values to be read. In this example, the values are aligned by inserting ``white space''. This is what is called comma-separated-values format (CSV). Function \Rfunction{write.csv()} and \Rfunction{read.csv()} can be used to write and read these files using the conventions used in this example.
\begin{verbatim}
 1.0, 24.5, 346, ABC
23.4, 45.6,  78, ZXY
\end{verbatim}

When reading a CSV file, white space is ignored and fields recognized based on separators. In most cases decimal points and exponential notation are allowed for floating point values. Alignment is optional, and helps only reading by humans, as white space is ignored. This miss-aligned version of the example above can be expected to be readable with base R function \Rfunction{read.csv()}.
\begin{verbatim}
1.0,24.5,346,ABC
23.4,45.6,78,ZXY
\end{verbatim}

With a fixed format for fields no delimiters are needed, but a description of the format is required. Decoding is based solely on the position of the characters in the line or record. A file like this cannot be interpreted without a description of the format used for saving the data. Files containing data stored in fixed format with fields can be read with base R function \Rfunction{read.fwf()}. Records, can be stored in multiple lines, each line with fields of different but fixed widths.
\begin{verbatim}
 10245346ABC
234456 78ZXY
\end{verbatim}

Function \Rfunction{read.fortran()} is a wrapper on \Rfunction{read.fwf()} that accepts format definitions similar to those used in FORTRAN, but not completely compatible with them. One particularity of FORTRAN \emph{formated data transfer} is that the decimal marker can be omitted in the saved file and its position specified as part of the format definition. Again an additional trick used to make text files (or stacks of punch cards) smaller.

R functions \Rfunction{write.table()} and \Rfunction{read.table()} default to separating fields with whitespace. Functions \Rfunction{write.csv()} and \Rfunction{read.csv()} have defaults for their arguments suitable for writing and reading CSV files in English-language locales. Functions \Rfunction{write.csv2()} and \Rfunction{read.csv2()} are similar have defaults for delimiters and decimal markers suitable for CSV files in locales with languages like Spanish, French, or Finnish that use comma (,) as decimal marker and semi-colon (;) as field delimiter. Another frequently used field delimiter is the ``tab'' or tabulator character, and sometimes any white space character (tab, space). In most cases the records (observations) are delimited by new lines, but this is not the only possible approach as the user can pass the delimiters to used as arguments in the function call.

We give examples of the use of all the functions described in the paragraphs above, starting by writing data to a file, and then reading this file back into the workspace. The \Rfunction{write()} functions take as argument data frames or objects that can be coerced into data frames. In contrast to \Rfunction{save()}, these functions can only write to files data that is in a tabular or matrix-like arrangement.

<<file-io-txt-01>>=
my1.df <- data.frame(x = 1:5, y = 5:1 / 10)
@

We write a CSV file suitable for an English language locale, and then display its contents. In most cases setting \code{row.names = FALSE} when writing a CSV file will help when it is read. Of course, if row names do contain important information, such as gene tags, you cannot skip writing the row names to the file unless you first copy these data into a column in the data frame. (Row names are stored separately as an attribute in \code{data.frame} objects.
<<file-io-txt-02>>=
write.csv(my.df, file = "my-file1.csv", row.names = FALSE)
file.show("my-file1.csv", pager = "console")
@

<<file-io-txt-02a, comment='', echo=FALSE>>=
cat(readLines('my-file1.csv'), sep = '\n')
@

If we had written the file using default settings, reading the file so as to recover the original objects, would have required overriding of the default argument for parameter \code{row.names}.
<<file-io-txt-02b>>=
my_read1.df <- read.csv(file = "my-file1.csv")
my_read1.df
all.equal(my.df, my_read1.df, check.attributes = FALSE)
@

\begin{playground}
Read the file with function \Rfunction{read.csv2()} instead of \Rfunction{read.csv()}. Although this may look as a waste of time, the point of the exercise is for you to get familiar with R's behaviour in case of such a mistake. This will help you recognize similar errors when they happen accidentally.
\end{playground}

We write a CSV file suitable for a Spanish, Finnish or similar locale, and then display its contents. It can be seen, that the same data frame is saved using different delimiters.
<<file-io-txt-03>>=
write.csv2(my.df, file = "my-file2.csv", row.names = FALSE)
file.show("my-file2.csv", pager = "console")
@

<<file-io-txt-03a, comment='', echo=FALSE>>=
cat(readLines('my-file2.csv'), sep = '\n')
@

As with \Rfunction{read.csv()} had we written row names to the file, we would have needed to override the default behaviour.
<<file-io-txt-03b>>=
my_read2.df <- read.csv2(file = "my-file2.csv")
my_read2.df
all.equal(my.df, my_read2.df, check.attributes = FALSE)
@

\begin{playground}
Read the file with function \Rfunction{read.csv()} instead of \Rfunction{read.csv2()}. This may look as an even more futile exercise than the previous one, but it isn't as the behaviour of R is different. Consider \emph{how} values are erroneously decoded in both exercises. If the \emph{structure} of the data frames read is not clear to you, do use function \Rfunction{str()} to look at them.
\end{playground}

We write a file with the fields separated by white space with function \Rfunction{write.table()}.
<<file-io-txt-04>>=
write.table(my.df, file = "my-file3.txt", row.names = FALSE)
file.show("my-file3.txt", pager = "console")
@

<<file-io-txt-04a, comment='', echo=FALSE>>=
cat(readLines('my-file3.txt'), sep = '\n')
@

In the case of \Rfunction{read.table()} there is no need to override the default, independently of row names are written to the file or not. The reason is related to the default behaviour of the \code{write} functions. Whether they write a column name (\code{""}, an empty character string) or not for the first column, containing the row names.
<<file-io-txt-04b>>=
my_read3.df <- read.table(file = "my-file3.txt", header = TRUE)
my_read3.df
all.equal(my.df, my_read3.df, check.attributes = FALSE)
@

\begin{playground}
If you are still unclear about why the files were decoded in the way they were, now try to read them with \code{read.table()}. Do now the three examples make sense to you?
\end{playground}

Function \Rfunction{cat()} takes R objects and writes them after conversion to character strings to a file, inserting one or more characters as separators, by default a space. This separator can be set by an argument through \code{sep}. In our example we set \code{sep} to a new line (entered as the escape sequence \code{"\\n"}.

<<file-io-txt-05>>=
my.lines <- c("abcd", "hello world", "123.45")
cat(my.lines, file = "my-file4.txt", sep = "\n")
file.show("my-file4.txt", pager = "console")
@

<<file-io-txt-05a, comment='', echo=FALSE>>=
cat(readLines('my-file4.txt'), sep = '\n')
@

<<file-io-txt-05b>>=
my_read.lines <- readLines('my-file4.txt')
my_read.lines
all.equal(my.lines, my_read.lines, check.attributes = FALSE)
@

\begin{warningbox}
There are couple of things to take into account when reading data from text files using base R functions \Rfunction{read.table()} and its relatives: by default columns containing character strings are converted into factors, and column names are sanitised (spaces and other ``inconvenient'' characters replaced with dots).
\end{warningbox}

\subsubsection[readr]{\pkgname{readr}}\label{sec:files:readr}

<<>>=
citation(package = "readr")
@

Package \pkgname{readr} is part of the \pkgname{tidyverse} suite. It defines functions that allow much faster input and output, and have different default behaviour. Contrary to base R functions, they are optimized for speed, but may sometimes wrongly decode their input and sometimes silently do this even for some CSV files that are correctly decoded by the base functions. Base R functions are dumb, the file format or delimiters must be supplied as arguments. The \pkgname{readr} functions use ``magic'' to guess the format, in most cases they succeed, which is very handy, but occasionally the power of the magic is not strong enough. The ``magic'' can be overridden by passing arguments. Another important advantage is that these functions read character strings formatted as dates or times directly into columns of class \code{datetime}.

All \code{write} functions defined in this package have an \code{append} parameter, which can be used to change the default behaviour of overwriting an existing file with the same name, to appending the output at its end.

Although in this section we exemplify the use of these functions by passing a file name as argument, URLs, and open file descriptors are also accepted. Furthermore, if the file name ends in a tag recognizable as indicating a compressed file format, the file will be uncompressed on-the-fly.

\begin{warningbox}
The names of functions ``equivalent'' to those described in the previous section have names formed by replacing the dot with an underscore, e.g.\ \Rfunction{read\_csv()} $\approx$ \Rfunction{read.csv()}. The similarity refers to the format of the files read, but not the order, names or roles of their formal parameters. Function \code{read\_table()} has a different behaviour to \Rfunction{read.table()}, although they both read fields separated by white space, \Rfunction{read\_table()} expects the fields in successive records (usually lines) to be vertically aligned while \Rfunction{read.table()} tolerates vertical misalignment. Other aspects of the default behaviour are also different, for example these functions do not convert columns of character strings into factors and row names are not set in the returned data frame (truly a \Rclass{tibble} which inherits from \Rclass{data.frame}).
\end{warningbox}

<<readr-01>>=
read_csv(file = "my-file1.csv")
@

<<readr-02>>=
read_csv2(file = "my-file2.csv")
@

Because of the vertically misaligned fields in file \code{my-file3.txt}, we need to use \Rfunction{read\_delim()} instead of \Rfunction{read\_table()}.
<<readr-03>>=
read_delim(file = "my-file3.txt", " ")
@

\begin{playground}
See what happens when you modify the code to use \code{read} functions to read files that are not matched to them---i.e.\ mix and match functions and files from the three code chunks above. As mentioned earlier forcing errors will help you learn how to diagnose when such errors are caused by coding mistakes.
\end{playground}

We demonstrate here the use of \Rfunction{write\_tsv()} to produce a text file with tab-separated fields.
<<readr-04>>=
write_tsv(my.df, path = "my-file5.tsv")
file.show("my-file5.tsv", pager = "console")
@

<<readr-04a, comment='', echo=FALSE>>=
cat(readLines('my-file5.tsv'), sep = '\n')
@

<<readr-04b>>=
my_read4.df <- read_tsv(file = "my-file5.tsv")
my_read4.df
all.equal(my.df, my_read4.df, check.attributes = FALSE)
@

We demonstrate here the use of \Rfunction{write\_excel\_csv()} to produce a text file with comma-separated fields suitable for reading with Excel.
<<readr-05>>=
write_excel_csv(my.df, path = "my-file6.csv")
file.show("my-file6.csv", pager = "console")
@

<<readr-05a, comment='', echo=FALSE>>=
cat(readLines('my-file6.csv'), sep = '\n')
@

\begin{playground}
Compare the output from \Rfunction{write\_excel\_csv()} and \Rfunction{write\_csv()}. What is the difference? Does it matter when you import the written CSV file into Excel (the version you are using, with the locale settings of your computer).
\end{playground}

<<readr-06>>=
write_lines(my.lines, path = "my-file7.txt")
file.show("my-file7.txt", pager = "console")
@

<<readr-06a, comment='', echo=FALSE>>=
cat(read_lines("my-file7.txt"), sep = '\n')
@

<<readr-06b>>=
my_read.lines <- read_lines("my-file7.txt")
my_read.lines
all.equal(my.lines, my_read.lines, check.attributes = FALSE)
@

Additional write and read functions not mentioned are also provided by the package: \Rfunction{write\_csv()}, \Rfunction{write\_delim()}, \Rfunction{write\_file()}, and \code{read\_fwf()}.

\begin{advplayground}
Use \Rfunction{write\_file()} to write a file that can be read with \Rfunction{read\_csv()}.
\end{advplayground}

\subsection{Worksheets}\label{sec:files:worksheets}

Microsoft Office, Open Office and Libre Office are the most frequently used suites containing programs based on the worksheet paradigm. There is available a standardized file format for exchange of worksheet data, but it does not support all the features present in native file formats. We will start by considering MS-Excel. The file format used by Excel has changed significantly over the years, and old formats tend to be less well supported by available R packages and may require the file to be updated to a more modern format with Excel itself before import into R. The current format is based on XML and relatively simple to decode, older binary formats are more difficult. Consequently for the format currently in use, there are alternatives.

\subsubsection{Exporting CSV files}

If you have access to the original software used, then exporting a worksheet to a text file in CSV format and importing it into R using the functions described in section \ref{sec:files:txt} starting on page \pageref{sec:files:txt} is a workable solution. It is not ideal from the perspective of storing the same data set repeatedly, which, can lead to these versions diverging when updated. A better approach is to, when feasible, to import the data directly from the workbook or worksheets into R.

\subsubsection['readxl']{\pkgname{readxl}}\label{sec:files:excel}

<<readxl-00>>=
citation(package = "readxl")
@

This package exports only two functions for reading Excel workbooks in xlsx format. The interface is simple, and the package easy to instal. We will import a file that in Excel looks as in the screen capture below.

\begin{center}
\includegraphics[width=0.75\textwidth]{figures/Book1-xlsx.png}
\end{center}

We first list the sheets contained in the workbook file with \Rfunction{excel\_sheets()}.
<<readxl-01>>=
sheets <- excel_sheets("extdata/Book1.xlsx")
sheets
@

In this case the argument passed to \code{sheet} is redundant, as there is only a single worksheet in the file. It is possible to use either the name of the sheet or a positional index (in this case \code{1} would be equivalent to \code{"my data"}). We use function \Rfunction{read\_excel()} to import the worksheet.
<<readxl-02>>=
Book1.df <- read_excel("extdata/Book1.xlsx", sheet = "my data")
Book1.df
@

Of the remaining arguments, \code{skip} is useful when we need to skip the top row of a worksheet.

\subsubsection['xlsx']{\pkgname{xlsx}}

Package \pkgname{xlsx} can be more difficult to install as it uses Java functions to do the actual work. However, it is more comprehensive, with functions both for reading and writing Excel worksheet and workbooks, in different formats. It also allows selecting regions of a worksheet to be imported.

<<xlsx-00>>=
citation(package = "xlsx")
@

Here we use function \Rfunction{read.xlsx()}, idexing the worksheet by name.

<<xlsx-01>>=
Book1_xlsx.df <- read.xlsx("extdata/Book1.xlsx", sheetName = "my data")
Book1_xlsx.df
@

As above, but indexing by a numeric index.

<<xlsx-02>>=
Book1_xlsx2.df <- read.xlsx2("extdata/Book1.xlsx", sheetIndex = 1)
Book1_xlsx2.df
@

With the three different functions we get a data frame or a tibble, which is compatible with data frames.
<<xlsx-03>>=
class(Book1.df)
class(Book1_xlsx.df)
class(Book1_xlsx2.df)
@

However, the columns are imported differently. Both \code{Book1.df} and \code{Book1\_xlsx.df} differ only in that the second column, a character variable, has been converted into a factor or not. This is to be expected as packages in the \pkgname{tidyverse} suite default to preserving character variables as such, while base R functions convert them to factors. The third function, \Rfunction{read.xlsx2()}, did not decode numeric values correctly, and converted everything into factors. This function is reported as being much faster than \Rfunction{read.xlsx()}.
<<xlsx-04>>=
sapply(Book1.df, class)
sapply(Book1_xlsx.df, class)
sapply(Book1_xlsx2.df, class)
@

With function \Rfunction{write.xlsx()} we can also write data frames out to Excel worksheets and even append new worksheets to an existing workbook.
<<xlsx-05>>=
set.seed(456321)
my.data <- data.frame(x = 1:10, y = 1:10 + rnorm(10))
write.xlsx(my.data, file = "extdata/my-data.xlsx", sheetName = "first copy")
write.xlsx(my.data, file = "extdata/my-data.xlsx", sheetName = "second copy", append = TRUE)
@

When opened in Excel we get a workbook, containing two worksheets, named using the arguments we passed through \code{sheetName} in the code chunk above.
\begin{center}
\includegraphics[width=0.75\textwidth]{figures/my-data-xlsx.png}
\end{center}

\begin{playground}
If you have some worksheet files available, import them into R, to get a feel of how the way data is organized in the worksheets affects how easy or difficult it is to read the data from them.
\end{playground}

\subsubsection['xml2']{\pkgname{xml2}}

Several modern data exchange formats are based on the XML standard format which uses schema for flexibility. Package \pkgname{xml2} provides functions for reading and parsing such files, as well as HTML files. This is a vast subject, of which I will only give a brief introduction.

We first read a very simple web page with function \Rfunction{read\_html()}.

<<xml2-01>>=
web_page <- read_html("http://r4photobiology.info/R/index.html")
html_structure(web_page)
@

And we extract the text from its \code{title} attribute, using functions \Rfunction{xml\_find\_all()} and \Rfunction{xml\_text()}.

<<xml2-02>>=
xml_text(xml_find_all(web_page, ".//title"))
@

The functions defined in this package and in package \pkgname{XML} can be used to ``harvest'' data from web pages, but also to read data from files using formats that are defined through XML schemas.

\subsection{Statistical software}\label{sec:files:stat}

There are two different comprehensive packages for importing data saved from other statistical such as SAS, Statistica, SPSS, etc. The long time ``standard'' the \pkgname{foreign} package and the much newer \pkgname{haven}. In the case of files saved with old versions of statistical programs, functions from \pkgname{foreign} tend to be more more robust than those from \pkgname{haven}.

\subsubsection[foreign]{\pkgname{foreign}}

Functions in this package allow to import data from files saved by several foreign statistical analysis programs, including \pgrmname{SAS}, \pgrmname{Stata} and \pgrmname{SPPS} among others, and a function for writing data into files with formats native to these three programs. Documentation is included with R describing them in \emph{R Data Import/Export}. As a simple example we use function \Rfunction{read.spss()} to read a \texttt{.sav} file, saved with a recent version of \pgrmname{SPSS}.

<<foreign-01>>=
my_spss.df <- read.spss(file = "extdata/my-data.sav", to.data.frame = TRUE)
head(my_spss.df)
@

Dates were not converted into R's datetime objects, but instead into numbers.

A second example, this time with a simple \code{.sav} file saved 15 years ago.

<<foreign-02>>=
thiamin.df <- read.spss(file = "extdata/thiamin.sav", to.data.frame = TRUE)
head(thiamin.df)
@

Another example, for a Systat file saved on an PC more than 20 years ago, and rea.

<<foreign-03>>=
my_systat.df <- read.systat(file = "extdata/BIRCH1.SYS")
my_systat.df
@

The functions in \pkgname{foreign} can return data frames, but not always this is the default.

\subsubsection[haven]{\pkgname{haven}}

The recently released package \pkgname{haven} is less ambitious in scope, providing read and write functions for only three file formats: \pgrmname{SAS}, \pgrmname{Stata} and \pgrmname{SPSS}. On the other hand \pkgname{haven} provides flexible ways to convert the different labelled values that cannot be directly mapped to normal R modes. They also decode dates and times according to the idiosyncrasies of each of these file formats. The returned \Rclass{tibble} objects in cases when the imported file contained labelled values needs some further work from the user before obtaining `normal' data-frame-compatible \Rclass{tibble} objects.

We here use function \Rfunction{read\_sav()} to import here a \code{.sav} file saved by a recent version of \pgrmname{SPSS}.

<<haven-01>>=
my_spss.tb <- read_sav(file = "extdata/my-data.sav")
my_spss.tb
head(my_spss.tb$harvest_date)
@

In this case the dates are correctly decoded.

And an \pgrmname{SPSS}'s \code{.sav} file saved 15 years ago.

<<haven-02>>=
thiamin.tb <- read_sav(file = "extdata/thiamin.sav")
thiamin.tb
thiamin.tb <- as_factor(thiamin.tb)
thiamin.tb
@

\begin{playground}
Compare the values returned by different \code{read} functions when applied to the same file on disk. Use \Rfunction{names()}, \Rfunction{str()} and \Rfunction{class()} as tools in your exploration. If you are brave, also use \Rfunction{attributes()}, \Rfunction{mode()}, \Rfunction{dim()}, \Rfunction{dimnames()}, \Rfunction{nrow()} and \Rfunction{ncol()}.
\end{playground}

\begin{playground}
If you use or have used in the past other statistical software or a general purpose language like \langname{Python}, look up some files, and import them into R.
\end{playground}

\subsection{NetCDF files}

In some fields including geophysics and meteorology NetCDF is a very common format for the exchange of data. It is also used in other contexts in which data is referenced to an array of locations, like with data read from Affymetrix micro arrays used to study gene expression. The NetCDF format allows the storage of metadata together with the data itself in a well organized and standardized format, which is ideal for exchange of moderately large data sets.

Officially described as
\begin{quote}
NetCDF is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.
\end{quote}

As sometimes NetCDF files are large, it is good that it is possible to selectively read the data from individual variables with functions in packages \pkgname{ncdf4} or \pkgname{RNetCDF}. On the other hand, this implies that contrary to other data file reading operations, reading a NetCDF file is done in two or more steps.

\subsubsection[ncdf4]{\pkgname{ncdf4}}

We first need to read an index into the file contents, and in additional steps we read a subset of the data. With \Rfunction{print()} we can find out the names and characteristics of the variables and attributes. In this example we use long term averages for potential evapotranspiration (PET).

We first open a connection to the file with function \Rfunction{nc\_open()}.

<<ncdf4-01>>=
meteo_data.nc <- nc_open("extdata/pevpr.sfc.mon.ltm.nc")
# very long output
# print(meteo_data.nc)
@

\begin{playground}
Uncomment the \Rfunction{print()} statement above and study the metadata available for the data set as a whole, and for each variable.
\end{playground}
The dimensions of the array data are described with metadata, mapping indexes to in our examples a grid of latitudes and longitudes and a time vector as a third dimension. The dates are returned as character strings. We get here the variables one at a time with function \Rfunction{ncvar\_get()}.

<<ncdf4-02>>=
time.vec <- ncvar_get(meteo_data.nc, "time")
head(time.vec)
longitude <-  ncvar_get(meteo_data.nc, "lon")
head(longitude)
latitude <- ncvar_get(meteo_data.nc, "lat")
head(latitude)
@

The \code{time} vector is rather odd, as it contains only month data as these are long-term averages. From the metadata we can infer that they correspond to the months of the year, and we directly generate these, instead of attempting a conversion.

We construct a \Rclass{tibble} object with PET values for one grid point, we can take advantage of \emph{recycling} or short vectors.

<<ncdf4-03>>=
pet.tb <-
    tibble(moth = month.abb[1:12],
           lon = longitude[6],
           lat = latitude[2],
           pet = ncvar_get(meteo_data.nc, "pevpr")[6, 2, ]
           )
pet.tb
@

If we want to read in several grid points, we can use several different approaches. In this example we take all latitudes along one longitude. Here we avoid using loops altogether when creating a \emph{tidy} \Rclass{tibble} object. However, because of how the data is stored, we needed to transpose the intermediate array before conversion into a vector.

<<ncdf4-04>>=
pet2.tb <-
    tibble(moth = rep(month.abb[1:12], length(latitude)),
           lon = longitude[6],
           lat = rep(latitude, each = 12),
           pet = as.vector(t(ncvar_get(meteo_data.nc, "pevpr")[6, , ]))
           )
pet2.tb
subset(pet2.tb, lat == latitude[2])
@

\begin{playground}
Play with \code{as.vector(t(ncvar\_get(meteo\_data.nc, "pevpr")[6, , ]))} until you understand what is the effect of each of the nested function calls, starting from \code{ncvar\_get(meteo\_data.nc, "pevpr")}. You will also want to use \Rfunction{str()} to see the structure of the objects returned at each stage.
\end{playground}

\begin{playground}
Instead of extracting data for one longitude across latitudes, extract data across longitudes for one latitude near the Equator.
\end{playground}

\subsubsection[RNetCDF]{\pkgname{RNetCDF}}

\begin{warningbox}
Package RNetCDF supports NetCDF3 files, but not those saved using the current NetCDF4 format.
\end{warningbox}

We first need to read an index into the file contents, and in additional steps we read a subset of the data. With \Rfunction{print.nc()} we can find out the names and characteristics of the variables and attributes. We open the connection with function \Rfunction{open.nc()}.

<<netcdf-01>>=
meteo_data.nc <- open.nc("extdata/meteo-data.nc")
str(meteo_data.nc)
# very long output
# print.nc(meteo_data.nc)
@

The dimensions of the array data are described with metadata, mapping indexes to in our examples a grid of latitudes and longitudes and a time vector as a third dimension. The dates are returned as character strings. We get variables, one at a time, with function \Rfunction{var.get.nc()}.

<<netcdf-02>>=
time.vec <- var.get.nc(meteo_data.nc, "time")
head(time.vec)
longitude <-  var.get.nc(meteo_data.nc, "lon")
head(longitude)
latitude <-  var.get.nc(meteo_data.nc, "lat")
head(latitude)
@

We construct a \Rclass{tibble} object with values for midday UV Index for 26 days. For convenience, we convert the strings into R's datetime objects.

<<netcdf-03>>=
uvi.tb <-
    tibble(date = ymd(time.vec, tz="EET"),
           lon = longitude[6],
           lat = latitude[2],
           uvi = var.get.nc(meteo_data.nc, "UVindex")[6,2,]
           )
uvi.tb
@

\subsection{Remotely located data}\label{sec:files:remote}

Many of the functions described above accept am URL address in place of file name. Consequently files can be read remotely, without a separate step. This can be useful, especially when file names are generated within a script. However, one should avoid, especially in the case of servers open to public access, not to generate unnecessary load on server and/or network traffic by repeatedly downloading the same file. Because of this, our first example reads a small file from my own web site. See section \ref{sec:files:txt} on page \pageref{sec:files:txt} for details of the use of these and other functions for reading text files.

<<url-01>>=
logger.df <-
      read.csv2(file = "http://r4photobiology.info/learnr/logger_1.txt",
                header = FALSE,
                col.names = c("time", "temperature"))
sapply(logger.df, class)
sapply(logger.df, mode)
@

<<url-02>>=
logger.tb <-
    read_csv2(file = "http://r4photobiology.info/learnr/logger_1.txt",
              col_names = c("time", "temperature"))
sapply(logger.tb, class)
sapply(logger.tb, mode)
@

While functions in package \pkgname{readr} support the use of URLs, those in packages \pkgname{readxl} and \pkgname{xlsx} do not. Consequently we need to first download the file writing a file locally, that we can read as described in section \ref{sec:files:excel} on page \pageref{sec:files:excel}.

<<url-11>>=
download.file("http://r4photobiology.info/learnr/my-data.xlsx",
              "data/my-data-dwn.xlsx",
              mode = "wb")
@

Functions in package \pkgname{foreign}, as well as those in package \pkgname{haven} support URLs. See section \ref{sec:files:stat} on page \pageref{sec:files:stat} for more information about importing this kind of data into R.

<<url-03>>=
remote_thiamin.df <-
  read.spss(file = "http://r4photobiology.info/learnr/thiamin.sav",
            to.data.frame = TRUE)
head(remote_thiamin.df)
@

<<url-04>>=
remote_my_spss.tb <-
    read_sav(file = "http://r4photobiology.info/learnr/thiamin.sav")
remote_my_spss.tb
@

Function \Rfunction{download.file()} in R's default \pkgname{utils} package can be used to download files using URLs. It supports differemt modes such as binary or text, and write or append, and different methods such as internal, wget and libcurl.

In this example we use a downloaded NetCDF file of long-term means for potential evapotranspiration from NOOA, the same used above in the \pkgname{ncdf4} example. This is a moderately large file at 444~KB. In this case we cannot directly open the connection to the NetCDF file, we first download it (commented out code, as we have a local copy), and then we open the local file.

<<url-05,eval=-2>>=
my.url <- paste("ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis.derived/",
                "surface_gauss/pevpr.sfc.mon.ltm.nc",
                sep = "")
#download.file(my.url,
#              mode = "wb",
#              destfile = "extdata/pevpr.sfc.mon.ltm.nc")
pet_ltm.nc <- nc_open("extdata/pevpr.sfc.mon.ltm.nc")
@

\begin{warningbox}
For portability NetCDF files should be downloaded in binary mode, setting \code{mode = "wb"}, which is required at least under MS-Windows.
\end{warningbox}

\subsection{Data acquisition from physical devices}\label{sec:data:acquisition}

Numerous modern data acquisition devices based on microcontrolers, including internet-of-things (IoT) devices, have servers (or daemons) that can be queried over a network connection to retrieve either real-time or looged data. Formats based on XML schemas or in JSON format are commonly used.

\subsubsection[jsonlite]{\pkgname{jsonlite}}

We give here a simple example using a module from the \href{http://www.yoctopuce.com/}{YoctoPuce} family using a software hub running locally. We retrieve logged data from a YoctoMeteo module.

\begin{infobox}
This example is not run, and needs setting the configuration of the YoctoPuce module beforehand. Fully reproducible examples, including configuration instructions, will be included in a future revision of the manuscript.
\end{infobox}

Here we use function \Rfunction{fromJSON()} to retrieve logged data from one sensor.

<<iot-01,eval=FALSE>>=
hub.url <- "http://127.0.0.1:4444/"
Meteo01.lst <-
    fromJSON(paste(hub.url, "byName/Meteo01/dataLogger.json",
                   sep = ""))
names(Meteo01.lst)
Meteo01.lst
@

The minimum, mean and maximum values for each logging interval, need to be split from a single vector. We do this by indexing with a logical vector (recycled). The data returned is \emph{tidy} with respect to the variables, with quantity names and units also returned by the module, as well as the time.

<<iot-02,eval=FALSE>>=
    val.vector <- unlist(Meteo01.lst[["val"]])
    dplyr::transmute(Meteo01.lst,
                     utc.time = as.POSIXct(utc, origin = "1970-01-01", tz = "UTC"),
                     qty = qty.name,
                     unit = qty.unit,
                     minimum = val.vector[c(TRUE, FALSE, FALSE)],
                     mean = val.vector[c(FALSE, TRUE, FALSE)],
                     maximum = val.vector[c(FALSE, FALSE, TRUE)],
                     dur,
                     freq)
@

\subsection{Databases}\label{sec:data:db}

One of the advantages of using databases is that subsets of cases and variables can be retrieved from databases, even remotely, making it possible to work both locally and remotely with huge data sets. One should remember that R natively keeps whole objects in RAM, and consequently available machine memory limits the size of data sets with which it is possible to work.

\begin{infobox}
The contents of this section is still missing, but will in any case be basic. I recomend the book \citetitle{Wickham2017} \autocite{Wickham2017} for learning how to use the packages in the \pkgname{tidyverse} suite, especially in the case of connecting to databases.
\end{infobox}

\section[Apply functions]{\emph{Apply} functions}\label{sec:data:apply}

\emph{Apply} functions apply functions to elements in a collection of R objects. These collections can be vectors, lists, data frames, matrices of arrays. As long as the operations to be applied are \emph{independent---i.e.\ the results from one iteration are not used in another iteration, and each iteration refers only one member of the collection of objects---} these functions can replace \code{for}, \code{while} or \code{repeat} loops.

\begin{explainbox}
\textbf{When apply functions cannot replace traditional loop constructs?} We will give some typical examples. First case is the accumulation pattern, where we ``walk'' through a collection storing a partial result between iterations.
<<not-apply-01>>=
set.seed(123456)
a.vector <- runif(20)
total <- 0
for (i in seq(along.with = a.vector)) {
  total <- total + a.vector[i]
  }
total
@

Although the loop above cannot the replaced by a statement based on an \emph{apply} function, it can be replaced by the summation function \Rfunction{sum()} from base R.
<<not-apply-02>>=
set.seed(123456)
a.vector <- runif(20)
total <- sum(a.vector)
total
@

Another frequent pattern are operations, at each iteration, on a subset composed by two or more consecutive elements of the collection. The simplest and probably most frequent calculation of this kind is the calculation of differences between successive members.
<<not-apply-03>>=
set.seed(123456)
a.vector <- runif(20)
b.vector <- numeric(length(a.vector) - 1)
for (i in seq(along.with = b.vector)) {
  b.vector[i] <- a.vector[i + 1] - a.vector[i]
  }
b.vector
@

In this case, we can use \code{diff()} instead of an explicit loop.
<<not-apply-04>>=
b.vector <- diff(a.vector)
b.vector
@
\end{explainbox}

\subsection{Base R's apply functions}

Base R's \emph{apply} functions differ on the class of the returned value and on the class of the argument expected for their \code{X} parameter: \Rfunction{apply()} expects a \code{matrix} or \code{array} as argument, or an argument like a \code{data.frame} which can be converted to a matrix or array. \Rfunction{apply()} returns an array or a list or a vector depending on the size, and consistency in length and class among the values returned by the applied function. \Rfunction{lapply()} and \Rfunction{sapply()} expect a \code{vector} or \code{list} as argument passed through \code{X}. \Rfunction{lapply()} returns a \code{list} or an \code{array}; and \Rfunction{vapply()} always \emph{simplifies} its returned value into a vector, while \Rfunction{sapply()} does the simplification according to the argument passed to its \code{simplify} parameter. All these \emph{apply} functions can be used to apply any R function that returns a value of the same or a different class as its argument. In the case of \Rfunction{apply()} and \Rfunction{lapply()} not even the length of the values returned for each member of the collection passed as argument, needs to be consistent. In summary, \Rfunction{apply()} is used to apply a function to the elements of an object that has \emph{dimensions} defined, and \Rfunction{lapply()} and \Rfunction{sapply()} to apply a function to the members of and object without dimensions, such as a vector.

\begin{explainbox}
Of course, a \code{matrix} can have a single row, a single column, or even a single element, but even in such cases, a \code{matrix} will have \emph{dimensions} defined and stored as an attribute.

<<dimensions-box-01>>=
my.vector <- 1:6
dim(my.vector)
@

<<dimensions-box-02>>=
one.col.matrix <- matrix(1:6, ncol = 1)
dim(one.col.matrix)
two.col.matrix <- matrix(1:6, ncol = 2)
dim(two.col.matrix)
one.elem.matrix <- matrix(1, ncol = 1)
dim(one.elem.matrix)
@

\begin{playground}
Print the matrices defined in the chucks above. Then, look up the help page for \Rfunction{array()} and write equivalent examples for arrays with three and higher dimensions.
\end{playground}
\end{explainbox}

We first examplify the use of \Rfunction{lapply()} and \Rfunction{sapply()} given their simpler argument for \code{X}.
<<apply-01>>=
set.seed(123456)
a.vector <- runif(10)
my.fun <- function(x, k) {log(x) + k}
z <- lapply(X = a.vector, FUN = my.fun, k = 5)
class(z)
dim(z)
z
z <- sapply(X = a.vector, FUN = my.fun, k = 5)
class(z)
dim(z)
z
z <- sapply(X = a.vector, FUN = my.fun, k = 5, simplify = FALSE)
class(z)
dim(z)
z
@

Anonymous functions can be defined on the fly, resulting in the same returned value.
<<apply-02>>=
sapply(X = a.vector, FUN = function(x, k) {log(x) + k}, k = 5)
@

Of course, as discussed in Chapter \ref{chap:????}, when vectorization is possible, this results also in fastest execution and simplest code.
<<apply-03>>=
log(a.vector) + 5
@

Next we give examples of the use of \Rfunction{apply()}. The argument passed to \code{MARGIN} determines, the dimension along which the matrix or array passed to \code{X} will be split before passing it as argument to the function passed through \code{FUN}. In the example below we get either row- or column means. In these examples, \Rfunction{sum()} is passed a vector, for each row or each column of the matrix. As function \Rfunction{sum()} returns a single value independently of the length of its argument, instead of a matrix, the returned value is a vector. In other words, an array with one dimension less than that of its input.

<<apply-04>>=
set.seed(123456)
a.mat <- matrix(runif(10), ncol = 2)
row.means <- apply(X = a.mat, MARGIN = 1, FUN = mean, na.rm = TRUE)
class(row.means)
dim(row.means)
row.means
col.means <- apply(X = a.mat, MARGIN = 2, FUN = mean, na.rm = TRUE)
class(col.means)
dim(col.means)
col.means
@

\begin{playground}
Look up the help pages for \Rfunction{apply()} and \Rfunction{mean()} to study them until you understand how to pass additional arguments to any applied function. Can you guess why \code{apply} was designed to have parameter names fully in upper case, something very unusual for R functions?
\end{playground}

\begin{warningbox}
If we apply a function that returns a value of the same length as its input, then the dimensions of the value returned by \Rfunction{apply()} are the same as those of its input. We use in the next examples a ``no-op'' function that returns its argument unchanged, so that input and output can be easily compared.

<<apply-05>>=
set.seed(123456)
a.mat <- matrix(1:10, ncol = 2)
no_op.fun <- function(x) {x}
b.mat <- apply(X = a.mat, MARGIN = 2, FUN = no_op.fun)
class(b.mat)
dim(b.mat)
b.mat
t(b.mat)
@

In the chunk above we passed \code{MARGIN = 2}, but if we pass \code{MARGIN = 1}, we get an equivalent return value but transposed! We use in the next example a ``no-op'' function than simply returns its argument unchanged, so that input and output can be easily compared. To restore the original layout of the matrix we can transpose the result with function \Rfunction{t()}.

<<apply-05a>>=
b.mat <- apply(X = a.mat, MARGIN = 1, FUN = no_op.fun)
class(b.mat)
dim(b.mat)
b.mat
t(b.mat)
@

Of course, these two toy examples, are something that can, and should be always avoided, as vectorization allows us to directly apply the function to the whole matrix.

<<apply-05b>>=
b.mat <- no_op.fun(a.mat)
@

A more realistic example, but difficult to grasp without seeing the toy examples shown above, is when we apply a function that returns a value of a different length than its input, but longer than one. If this length is consistent, an array with matching dimensions is returned, but again with the original columns as rows. What happens is that by using \Rfunction{apply()} one dimension of the original matrix or array disappears, as we apply the function over it. Consequently, given how matrices are stored in R, when the column dimension disappears, the row dimension becomes the new column dimension. After this, the elements of the vectors returned by the applied function applied, are stored along rows. To restore the original rows to rows in the result matrix we can transpose the it with function \Rfunction{t()}.

<<apply-06>>=
set.seed(123456)
a.mat <- matrix(runif(10), ncol = 2)
mean_and_var <- function(x, na.rm = FALSE) {
       c(mean(x, na.rm = na.rm),  var(x, na.rm = na.rm))
    }
c.mat <- apply(X = a.mat, MARGIN = 1, FUN = mean_and_var, na.rm = TRUE)
class(c.mat)
dim(c.mat)
c.mat
t(c.mat)
@

In this case, calling the user-defined function with the whole matrix as argument is not equivalent. Of course, a \code{for} loop stepping through the rows would be the job, but more slowly.

\end{warningbox}

Function \Rfunction{vapply()} is not as frequently used, but can be sometimes useful. Here is a possible way of obtaining means and variances across member vectors at each vector index position from a list of vectors. These could be called \emph{parallel} means and variances.

<<apply-07>>=
set.seed(123456)
a.list <- lapply(rep(4, 5), runif)
a.list
mean_and_var <- function(x, na.rm = FALSE) {
       c(mean(x, na.rm = na.rm),  var(x, na.rm = na.rm))
    }
values <- vapply(X = a.list,
                 FUN = mean_and_var,
                 FUN.VALUE = c(mean = 0, var = 0),
                 na.rm = TRUE)
class(values)
dim(values)
values
@

\section[Grammar of data manipulation]{The grammar of data manipulation of the \pkgname{tidyverse}}

Packages in \code{tidyverse}, define more user-friendly \emph{apply} functions, which I describe in the next sections. These packages, do much more than providing replacements for R's \emph{apply} functions. They define a ``grammar of data'' for data manipulations like transformations and summaries, based on the same philosophy as that behind the grammar of graphics on which package \pkgname{ggplot2} is based (see Chapter \ref{chap:R:plotting} starting on page \pageref{chap:R:plotting}).

To make the problem of manipulating data, tractable and consistent, the first step is to settle on a certain way of storing data. In R's data frames, variables are most frequently in columns and cases are in rows. This is a good start and also frequently used in other software. The first major inconsistency across programs, and to some extent among R packages, is how to store data for sequential or repeated measurements. Do the rows represent measuring events, or measured objects? In R, data from individual measuring events are in most cases stored as rows, and if those that correspond to the same object or individual encoded with an index variable. Furthermore, say in a time sequence, the times or dates are stored in an additional variable. R's approach is much more flexible in that it does not assume that observations on different individuals are synchronized. \citeauthor{Wickham2014a} \cite{Wickham2014a} has coined the name ``tidy data'' organized in this manner.

Hadley Wickham, together with collaborators, has developed a set of R tools for the manipulation, plotting and analysis of \emph{tidy data}, thoroughly described in the recently published book \citetitle{Wickham2017} \autocite{Wickham2017}. The book \citetitle{Peng2017} \autocite{Peng2017} covers data manipulaiton in the first chapters before moving on to programming. Here we give an overview of the components of the \pkgname{tidyverse} grammar of data manipulation. The book \citetitle{Wickham2017} and the documentation included with the various packages should be consulted for a deeper and more detailed discussion. Aspects of the \pkgname{tidyverse} related to reading and writing data files (\pkgname{readr}, \pkgname{readxl}, and \pkgname{xml2}) have been discussed in earlier sections of this chapter, while the use of (\pkgname{ggplot2}) for plotting is described in later chapters.

\subsection{Better data frames}

Package \pkgname{tibble} defines an improved class \Rclass{tibble} that can be used in place of data frames. Changes are several, including differences in default behaviour of both constructors and methods. Objects of class \Rclass{tibble} can non-the-less be used as arguments for most functions that expect data frames as input.

\begin{infobox}
In their first incarnation, the name for \Rclass{tibble} was \code{data\_frame} (with a dash instead of a dot). The old name is still recognized, but it is better to only use \Rfunction{tibble()} to avoid confusion. One should be aware that although the constructor \Rfunction{tibble()} and conversion function \Rfunction{as.tibble()}, as well as the test \Rfunction{is.tibble()} use the name \Rclass{tibble}, the class attribute is named \code{tbl}.

<<tibble-info-01>>=
my.tb <- tibble(numbers = 1:3)
is.tibble(my.tb)
class(my.tb)
@

Furthermore, by necessity, to support tibbles based on different underlying data sources a further derived class is needed. In our example, as our tibble has an underlying \code{data.frame} class, the most derived class of \code{my.tb} is \code{tbl\_df}.
\end{infobox}

We start with the constructor and conversion methods. For this we will define our own diagnosis function.

<<tibble-01>>=
show_classes <- function(x) {
  cat(
    paste(paste(class(x)[1],
    "containing:"),
    paste(names(x),
          sapply(x, class), collapse = ", ", sep = ": "),
    sep = "\n")
    )
}
@

In the next two chunks we can see some of the differences. The \Rfunction{tibble()} constructor does not by default convert character data into factors, while the \Rfunction{data.frame()} constructor does.

<<tibble-02>>=
my.df <- data.frame(codes = c("A", "B", "C"), numbers = 1:3, integers = 1L:3L)
is.data.frame(my.df)
is.tibble(my.df)
show_classes(my.df)
@

Tibbles are data frames---or more formally class \Rclass{tibble} is derived from class \code{data.frame}. However, data frames are not tibbles.

<<tibble-03>>=
my.tb <- tibble(codes = c("A", "B", "C"), numbers = 1:3, integers = 1L:3L)
is.data.frame(my.tb)
is.tibble(my.tb)
show_classes(my.tb)
@

The \Rfunction{print()} method for tibbles, overrides the one defined for data frames.

<<tibble-04>>=
print(my.df)
print(my.tb)
@

\begin{playground}
The main difference is in how tibbles and data frames are printed when they have many rows. Construct a data frame and an equivalent tibble with at least 50 rows, and then test how the output looks when they are printed.
\end{playground}

Data frames can be converted into tibbles with \code{as.tibble()}.

<<tibble-05>>=
my_conv.tb <- as.tibble(my.df)
is.data.frame(my_conv.tb)
is.tibble(my_conv.tb)
show_classes(my_conv.tb)
@

<<tibble-06>>=
my_conv.df <- as.data.frame(my.tb)
is.data.frame(my_conv.df)
is.tibble(my_conv.df)
show_classes(my_conv.df)
@

\begin{playground}
Look carefully at the result of the conversions. Why do we now have a data frame with \code{A} as \code{character} and tibble with \code{A} as a \code{factor}?
\end{playground}

\begin{explainbox}
Not all conversion functions work consistently when converting from a derived class into its parent. The reason for this is disagreement between author on what is the \emph{correct} behaviour based on logic and theory. You are not likely to be hit by this problem frequently, but it can be difficult to diagnose.

We have already seen that calling \Rfunction{as.data.frame()} on a tibble strips the derived class attributes, returning a data frame. We now look at the whole contents on the \code{"class"} attribute to better exemplify the problem. We also test the two objects for equality, in two different ways. Using the operator \code{==} tests for equivalent objects. Objects that contain the same data. Using \Rfunction{identical()} tests that objects are exactly the same, including same attributes, including same equal class attributes.

<<tibble-box-01>>=
class(my.tb)
class(my_conv.df)
my.tb == my_conv.df
identical(my.tb, my_conv.df)
@

Now we derive from a tibble, and then attempt a conversion back into a tibble.

<<tibble-box-02>>=
my.xtb <- my.tb
class(my.xtb) <- c("xtb", class(my.xtb))
class(my.xtb)
my_conv_x.tb <- as_tibble(my.xtb)
class(my_conv_x.tb)
my.xtb == my_conv_x.tb
identical(my.xtb, my_conv_x.tb)
@

The two viewpoints on conversion functions are as follows. 1) The conversion function should return an object of its corresponding class, even if the argument is an object of a derived class, stripping the derived class. 2) If the object is of the class to be converted to, including objects of derived classes, then it should remain untouched. Base R follows, as far as I have been able to work out, approach 1). Packages in the \pkgname{tidyverse} follow approach 2). If in doubt about the behaviour of some function, then you need to do a test similar to the I have presented in the chunks in this box.
\end{explainbox}

There are additional important differences between the constructors \Rfunction{tibble()} and \Rfunction{data.frame()}. One of them is that variables (``columns'')  being defined can be used in the definition of subsequent variables.

<<tibble-07>>=
tibble(a = 1:5, b = 5:1, c = a + b, d = letters[a + 1])
@

\begin{playground}
What is the behaviour if you replace \Rfunction{tibble()} by \Rfunction{data.frame()} in the statement above?
\end{playground}

Furthermore, while data frame columns are required to be vectors, columns of tibbles can also be lists.

<<tibble-08>>=
tibble(a = 1:5, b = 5:1, c = list("a", 2, 3, 4, 5))
@

Which even allows a list of lists as a variable, or a list of vectors.

<<tibble-09>>=
tibble(a = 1:5, b = 5:1, c = list("a", 1:2, 0:3, letters[1:3], letters[3:1]))
@

\subsection{Tidying up data}

In later sections of this and subsequent chapters we assume that available data is in a tidy arrangement, in which rows correspond to measurement events, and columns correspond to values for different variables measured at a given measuring event, or descriptors of groups or permanent features of the measured units. Real-world data can be quite messy, so frequently the first task in an analysis is to make data in ad-hoc or irregular formats ``tidy''. Please consult the vignette other documentation of package \pkgname{tidyr} for details.

In most cases using function \Rfunction{gather()} is the easiest way of converting data in a ``wide'' form into data into ``long'' form, or \emph{tidy} format. We will use the \code{iris} data set included with R. We print \code{iris} as a tibble for the nicer formatting of the screen output, but we do not save the result. We use \code{gather} to obtain a long-form tibble. Be aware that in this case, the original wide form would in some cases be best for further analysis.

We first convert \code{iris} into a tibble to more easily control the length of output.

<<tidy-tibble-01>>=
data(iris)
iris.tb <- as.tibble(iris)
iris.tb
@

By comparing \code{iris.tb} above with \code{long\_iris} below we can appreciate how \Rfunction{gather()} transformed its input.

<<tidy-tibble-01a>>=
long_iris <- gather(iris.tb, key = part, value = dimension, -Species)
long_iris
@

\begin{playground}
To better understand why I added \code{-Species} as an argument, edit the code removing it, and execute the statement to see how the returned tibble is different.
\end{playground}

\subsection{Row-wise manipulations}

We can calculate derived quantities by combining different variables measured on the same measuring unit---i.e.\ calculations within a single row of a data frame or tibble. In this case there are two options, we add new variables (columns) retaining existing ones using \Rfunction{mutate()} or we assemble a new tibble containing only the columns we explicitly specify using \Rfunction{transmute()}.

Continuing with the example from the previous section, we most likely would like to split the values in variable \code{part} into \code{plant\_part} and \code{part\_dim}. We use \code{mutate()} from \pkgname{dplyr} and \Rfunction{str\_extract()} from \pkgname{stringr}. We use regular expressions as arguments passed to \code{pattern}.  We do not show it here, but \Rfunction{mutate()} can be used with variables of any \code{mode}, and calculations can involve values from several columns. It is even possible to operate on values applying a lag or in other words using rows displaced relative to the current one. As shown in the example in section \ref{sec:dataex:birch} on page \pageref{sec:dataex:birch}, within a single call to \Rfunction{mutate()} values calculated first can be used in the calculations for later variables.

<<tidy-tibble-02>>=
long_iris <- mutate(long_iris,
                    plant_part = str_extract(part, "^[:alpha:]*"),
                    part_dim = str_extract(part, "[:alpha:]*$"))
long_iris
@

In the next few chunks we print the returned values rather than saving then in variables. In most cases in practice one will combine these function into a ``pipe'' using operator \Roperator{\%>\%} (see section \ref{sec:data:pipes} on page \pageref{sec:data:pipes}, and for more realistic examples, section \ref{sec:dataex} starting on page \pageref{sec:dataex}).

Function \Rfunction{arrange()} is used for sorting the rows---makes sorting a data frame simpler than by using \Rfunction{sort()} and \Rfunction{order()}. These two base R methods are more versatile.

<<tidy-tibble-03>>=
arrange(long_iris, Species, plant_part, part_dim)
@

Function \Rfunction{filter()} to select a subset of rows---similar to \Rfunction{subset()} but with a syntax consistent with that of other functions in the \pkgname{tidyverse}.

<<tidy-tibble-04>>=
filter(long_iris, plant_part == "Petal")
@

Function \Rfunction{slice()} to select a subset of rows based on their positions---would be done with positional indexes with \code{[ , ]} in base R.

<<tidy-tibble-05>>=
slice(long_iris, 1:5)
@

Function \Rfunction{select()} to select a subset of columns---requires selection with subindexes in base R. In the first example we remove one column by name.

<<tidy-tibble-06>>=
select(long_iris, -part)
@

In addition \Rfunction{select()} as other functions in \pkgname{dplyr} can be used together with functions \Rfunction{starts\_with()}, \Rfunction{ends\_with()}, \Rfunction{contains()}, and \Rfunction{matches()} to select groups of columns to be selected to be retained or removed. For this example we use R's \code{iris} instead of our \code{long\_iris}.

<<tidy-tibble-06a>>=
select(iris.tb, -starts_with("Sepal"))
@

<<tidy-tibble-06b>>=
select(iris.tb, Species, matches("pal"))
@

Function \Rfunction{rename()} to rename columns---requires the use of \Rfunction{names()} and \Rfunction{names<-()} and a way of matching the old name in base R.

<<tidy-tibble-07>>=
rename(long_iris, dim = dimension)
@

The first advantage a user sees of these functions is the completeness of the set of operations supported and the symmetry and consistency among the different functions. A second advantage is that almost all the functions are defined not only for objects of class \Rclass{tibble}, but also for objects of class \code{data.table} and for accessing SQL based databases with the same syntax. The functions are also optimized for fast performance.

\subsection{Group-wise manipulations}

Another important operation is to summarize quantities by group of rows. Contrary to base R, the grammar of data manipulation, splits this operation in two: the setting of the grouping, and the calculation of summaries. This simplifies the code, making it more easily understandable, compared to the approach of base R's \Rfunction{aggregate()}, and it also makes it easier to summarize several columns in a single operation.

The first step is to use \Rfunction{group\_by()} to ``tag'' a tibble with the grouping. We create a \emph{tibble} and then convert it into a \emph{grouped tibble}.

<<tibble-grouped-01>>=
my.tb <- tibble(numbers = 1:9, letters = rep(letters[1:3], 3))
my_gr.tb <- group_by(my.tb, letters)
@

Once we have a grouped tibble, function \Rfunction{summarise()} will recognize the grouping and use it when the summary values are calculated.

<<tibble-grouped-02>>=
summarise(my_gr.tb,
          mean_numbers = mean(numbers),
          median_numbers = median(numbers),
          n = n())
@

\begin{explainbox}
\textbf{How is grouping implemented for data-frame-based tibbles?} In our case as our tibble belongs to class \code{tibble\_df}, grouping adds \code{grouped\_df} as the most derived class. It also adds several attributes with the grouping information in a format suitable for fast selection of group members.

<<tibble-grouped-box-01>>=
my.tb <- tibble(numbers = 1:9, letters = rep(letters[1:3], 3))
class(my.tb)
my_gr.tb <- group_by(my.tb, letters)
class(my_gr.tb)
@

\begin{playground}
Use function \Rfunction{attributes()} to compare the attributes of  \code{my.tb} and \code{my\_gr.tb}. Trysee how the groups information is stored in
\end{playground}
\end{explainbox}


\section{Pipes and tees}\label{sec:data:pipes}

Pipes have been part of Unix shells already starting from the early days of Unix in 1973. By the early 1980's the idea had led to the development of many \emph{tools} to be used in \pgrmname{sh} connected by pipes \autocite{Kernigham1981}. Shells developed more recently like the Korn shell, \pgrmname{ksh}, and \pgrmname{bash} maintained support for this approach \autocite{Rosenblatt1993}. The idea behind the concept of data pipe, is that one can directly use the output from one tool as input for the tool doing the next stage in the processing. These tools are simple programs that do a defined operation, such as \pgrmname{ls} or \pgrmname{cat}---from where the names of equivalent functions in \langname{R} were coined.

Apple's OS X is based on Unix, and allows the use of pipes at the command prompt and in shell scripts. Linux uses the tools from the Gnu project that to a large extent replicate and extend the capabilities  by the and also natively supports \emph{pipes} equivalent to those in Unix. In Windows support for pipes was initially partial at the command prompt. Currently, Window's PowerShell supports the use of pipes, as well as some Linux shells are available in versions that can be used under MS-Windows.

Within R code, the support for pipes is not native, but instead implemented by some recent packages. Most of the packages in the \code{tidyverse} support this new syntax through the use of package \pkgname{magrittr}. The use of pipes has advantages and disadvantages. They are at their best when connecting small functions with rather simple inputs and outputs. They tend, yet, to be difficult to debug, a problem that counterbalances the advantages of the clear and consice notation achieved.

\subsection{Pipes and tees}

The \emph{pipe} operator \Roperator{\%>\%} is defined in package \pkgname{magrittr}, but imported and re-exported by other packages in the \pkgname{tidyverse}. The idea is that the value returned by a function is passed by the pipe operator as the first argument to the next function in the ``pipeline''.

We can chain some of the examples in the previous section into a ``pipe''.

<<pipes-01>>=
tibble(numbers = 1:9, letters = rep(letters[1:3], 3)) %>%
  group_by(letters) %>%
  summarise(mean_numbers = mean(numbers),
            var_numbers = var(numbers),
            n = n())
@

I we want to save the returned value, to me it feels more natural to use a left to right assignment, although the usual right to left one can also be used.

<<pipes-02>>=
tibble(numbers = 1:9, letters = rep(letters[1:3], 3)) %>%
  group_by(letters) %>%
  summarise(mean_numbers = mean(numbers),
            var_numbers = var(numbers),
            n = n()) -> summary.tb
summary.tb
@

<<pipes-03>>=
summary.tb <-
    tibble(numbers = 1:9, letters = rep(letters[1:3], 3)) %>%
      group_by(letters) %>%
      summarise(mean_numbers = mean(numbers),
                var_numbers = var(numbers),
                n = n())
summary.tb
@

As \Rfunction{print()} returns its input, we can also include it in the middle of a pipe as a simple way of visualizing what takes place at each step.

<<pipes-04>>=
tibble(numbers = 1:9, letters = rep(letters[1:3], 3)) %>%
  print() %>%
  group_by(letters) %>%
  summarise(mean_numbers = mean(numbers),
            var_numbers = var(numbers),
            n = n())  %>%
            print() -> summary.tb
@

\begin{explainbox}
\textbf{Why and how we can insert a call to \Rfunction{print()} in the middle of a pipe?} An extremely simple example, with a twist, follows.

<<pipes-expl-01>>=
print("a") %>% print()
@

The example above is equivalent to.

<<pipes-expl-02>>=
print(print("a"))
@

The examples above are somehow surprising but instructive. Function \Rfunction{print()} returns a value, its first argument, but \emph{invisibly}---see help for \Rfunction{invisible()}. Otherwise default printing would result in the value being printed twice at the R prompt. We can demonstrate this by saving the value returned by print.

<<pipes-expl-03>>=
a <- print("a")
class(a)
a
b <- print(2)
class(b)
b
@

\end{explainbox}

\begin{playground}
Assemble different pipes, predict what will be the output, and check your prediction by executing the code.
\end{playground}

Although \Roperator{\%>\%} is the most frequently used pipe operator, there are some additional ones available. We start by creating a tibble.

<<pipes-11>>=
my.tb <- tibble(numbers = 1:9, letters = rep(letters[1:3], 3))
@

We first demonstrate that the pipe can have at its head a variable with the same operator as we used above, in this case a tibble.

<<pipes-12>>=
my.tb %>%
  group_by(letters) %>%
  summarise(mean_numbers = mean(numbers),
            var_numbers = var(numbers),
            n = n())
my.tb
@

We could save the output of the pipe to the same variable at the head of the pipe by explicitly using the same name, but operator \Roperator{\%<>\%} does this directly.

<<pipes-13>>=
my.tb %<>%
  group_by(letters) %>%
  summarise(mean_numbers = mean(numbers),
            var_numbers = var(numbers),
            n = n())
my.tb
@

A few additional operators defined in \pkgname{magrittr} are not re-exported by packages in the \pkgname{tidyverse}, so their use requires \pkgname{magrittr} to be loaded.

When functions have a side-effect like \Rfunction{print()} displaying its input and passing it unchanged as the returned value, we do not need to split flow of processing through a pipe. In real house plumbing, when a split is needed a ``tee'' shaped pipe joint is used. This is where the name tee as used in programming originates. Operator \Roperator{\%T>\%} passes along not the value returned by a function, but instead the value passed to it as input.

As in the previous chunk we assigned the summaries to \code{my.tb}, we need to re-create it to run the next example.

<<pipes-14>>=
my.tb <- tibble(numbers = 1:9, letters = rep(letters[1:3], 3))
@

<<pipes-15>>=
sump <- function(x) {print("hello"); return(NULL)}
my.tb %>%
  group_by(letters) %>%
  summarise(mean_numbers = mean(numbers),
            var_numbers = var(numbers),
            n = n()) %T>%
  sump() -> summary.tb
@

We can see that the value saved in \code{summary.tb} is the one returned by \Rfunction{summarize()} rather than the one returned by \Rfunction{sump()}.

\begin{playground}
Look up the help page for operator \Roperator{\%\$\%} and write an example of its use.
\end{playground}

\section{Joins}

Joins allow us to combine two data sources which share some variables. The variables in common are used to match the corresponding rows before adding columns from both sources together. There are several \emph{join} functions in \pkgname{dplyr}. They differ mainly in how they handle mismatched rows.

We create here some artificial data to demonstrate the use of these functions. We will create two small tibbles, with one column in common and one mismatched row in each.

<<joins-00>>=
first.tb <- tibble(idx = c(1:4, 5), values1 = "a")
second.tb <- tibble(idx = c(1:4, 6), values2 = "b")
@

Here we apply all the \emph{join} functions exported by \pkgname{dplyr}---\Rfunction{full\_join()}, \Rfunction{left\_join()}, \Rfunction{right\_join()}, \Rfunction{inner\_join()}, \Rfunction{semi\_join()}, and \Rfunction{anti\_join()}---to the two tibbles, each time swapping their order as input to help make the differences in behaviour clear.

<<joins-01>>=
full_join(first.tb, second.tb)
@

<<joins-01a>>=
full_join(second.tb, first.tb)
@

<<joins-02>>=
left_join(first.tb, second.tb)
@

<<joins-02a>>=
left_join(second.tb, first.tb)
@

<<joins-03>>=
right_join(first.tb, second.tb)
@

<<joins-03a>>=
right_join(second.tb, first.tb)
@

<<joins-04>>=
inner_join(first.tb, second.tb)
@

<<joins-04a>>=
inner_join(second.tb, first.tb)
@

<<joins-05>>=
semi_join(first.tb, second.tb)
@

<<joins-05a>>=
semi_join(second.tb, first.tb)
@

<<joins-06>>=
anti_join(first.tb, second.tb)
@

<<joins-06a>>=
anti_join(second.tb, first.tb)
@

See section \ref{sec:dataex:well:plate} on \pageref{sec:dataex:well:plate} for a realistic example of the use of a \emph{join}.

\section{Extended examples}\label{sec:dataex}

\subsection{Well-plate data}\label{sec:dataex:well:plate}

Our first example attempts to simulate data arranged in rows and columns based on spatial position, such as in a well plate. We will use pseudo-random numbers for the fake data---i.e.\ the measured response.

<<well-plate-data-01>>=
well_data.tb <-
  as.tibble(matrix(rnorm(50),
                   nrow = 5,
                   dimnames = list(as.character(1:5), LETTERS[1:10])))
# drops names of rows
well_data.tb <-
  add_column(well_data.tb, row_ids = 1:5, .before = 1)
@

In addition, we create a matrix of fake treatment ids.
<<well-plate-data-02>>=
well_ids.tb <-
  as.tibble(matrix(sample(letters, size = 50, replace = TRUE),
                   nrow = 5,
                   dimnames = list(as.character(1:5), LETTERS[1:10])))
# drops names of rows
well_ids.tb <-
  add_column(well_ids.tb, row_ids = 1:5, .before = 1)
@

As we will combine them, the coordinates should be encoded consistently in the two objects.
I will take the approach of first converting each tibble into a tidy tibble. We use function \Rfunction{gather()} from package \pkgname{tidyr}.

<<well-plate-data-03>>=
well_data.ttb <- gather(well_data.tb,
                       key = col_ids, value = reading,
                       -row_ids)
well_ids.ttb <- gather(well_ids.tb,
                       key = col_ids, value = group,
                       -row_ids)
@

Now we need to join the two tibbles into a single one. In this case, as we know that the row order in the two tibbles is matched, we could simply use \Rfunction{cbind()}. However, \Rfunction{full\_join()}, from package \pkgname{dplyr} provides a more general and less error prone alternative as it can do the matching based on the values of any variables common to both tibbles, by default all the variables in common, as needed here. We use a ``pipe'', through which, after the join, we remove the ids (assuming they are no longer needed), sort the rows by group, and finally save the result to a new ``tidy'' tibble.

<<well-plate-data-04>>=
full_join(well_ids.ttb, well_data.ttb) %>%
  select(-row_ids, -col_ids) %>%
  arrange(group) -> well.tb
well.tb
@

We finally calculate \emph{summaries} by group using function \Rfunction{summarise()}, and store the tibble containing the summaries to variable \code{well\_summaries.tb}.

<<well-plate-data-05>>=
group_by(well.tb, group) %>%
  summarise(avg_read = mean(reading),
            var_read = var(reading),
            count = n()) -> well_summaries.tb
well_summaries.tb
@

We now save the tibbles into an R data file with function \Rfunction{save()}.

<<well-plate-data-06>>=
save(well.tb, well_summaries.tb, file = "data/well-data.rda")
@

\subsection{Seedling morphology}\label{sec:dataex:birch}

We use here data from an experiment on the effects of spacing in the nursery between silver birch seedlings on their morphology. We take one variable from a lager study \autocite{Aphalo2006}, the leaf area at different heights above the ground in 10~cm increments. Area was measured separately for leaves on the main stem and leaves on branches.

In this case, as the columns are badly aligned in the original text file, we use \Rfunction{read.table()} from base R, rather than \Rfunction{read\_table()} from \pkgname{readr}. Afterwards we heavily massage the data into shape so as to obtain a tidy tibble with the total leaf area per height segment per plant. The file contains additional data that we discard for this example.

<<birch-area-01>>=
as.tibble(read.table("extdata/areatable.dat", header = TRUE)) %>%
  filter(row %in% 4:8) %>%
  select(code, tray, row, starts_with("a.")) %>%
  gather(key = sample, value = area, -tray, -row, -code) %>%
  mutate(segment = str_extract(sample, "[0-9]{1,2}"),
         part = ifelse(str_extract(sample, "[bm]") == "b",
                       "branch", "main")) %>%
  group_by(tray, code, row, segment) %>%
  summarise(area_tot = sum(area)) -> birch.tb
birch.tb
@

\begin{playground}
The previous chunk uses a long ``pipe'' to manipulate the data. I built this example interactively, starting at the top, and adding one line at a time. Repeat this process, line by line. If in a given line you do not understand why a certain bit of code is included, look at the help pages, and edit the code to experiment.
\end{playground}

We now will calculate means per true replicate, the trays. Then use these means to calculate overall means, standard deviations and coefficients of variabilities (\%).

<<birch-area-02>>=
group_by(birch.tb, tray, row, segment) %>%
  summarise(area = mean(area_tot)) %>%
  group_by(row, segment) %>%
  summarise(mean_area = mean(area),
            sd_area = sd(area),
            cv_area = sd_area / mean_area * 100) ->
  birch_summaries.tb
birch_summaries.tb
@

We could be also interested in total leaf area per plant. The code is the same as above, but with no grouping for \code{segment}.

<<birch-area-03>>=
group_by(birch.tb, tray, row) %>%
  summarise(area = mean(area_tot)) %>%
  group_by(row) %>%
  summarise(mean_area = mean(area),
            sd_area = sd(area),
            cv_area = sd_area / mean_area * 100) ->
  birch_plant_summaries.tb
birch_plant_summaries.tb
@

We now save the tibbles into an R data file.

<<birch-area-04>>=
save(birch.tb, birch_summaries.tb, birch_plant_summaries.tb,
     file = "data/birch-data.rda")
@

\begin{playground}
Repeat the same calculations for all the rows as I originally did. I eliminated the data from the borders of the trays, as those plants apparently did not really experience as crowded a space as that corresponding to the nominal spacing.
\end{playground}

\begin{infobox}
It is always good to clean up, and in the case of the book, the best way to test that the examples
can be run in a ``clean'' system.

<<clear-up-data-folders>>=
unlink("./data", recursive = TRUE)
unlink("./extdata", recursive = TRUE)
@

\end{infobox}

<<eco=FALSE>>=
try(detach(package:jsonlite))
try(detach(package:lubridate))
try(detach(package:ncdf4))
try(detach(package:RNetCDF))
try(detach(package:xml2))
try(detach(package:haven))
try(detach(package:foreign))
try(detach(package:xlsx))
try(detach(package:readxl))
try(detach(package:readr))
try(detach(package:tidyr))
try(detach(package:dplyr))
try(detach(package:stringr))
try(detach(package:magrittr))
try(detach(package:tibble))
try(detach(package:learnrbook))
@

